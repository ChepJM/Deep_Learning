{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for (0, 'W')\n",
      "Gradient check passed!\n",
      "Checking gradient for (0, 'B')\n",
      "Gradient check passed!\n",
      "Checking gradient for (2, 'W')\n",
      "Gradient check passed!\n",
      "Checking gradient for (2, 'B')\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for (0, 'W')\n",
      "Gradient check passed!\n",
      "Checking gradient for (0, 'B')\n",
      "Gradient check passed!\n",
      "Checking gradient for (2, 'W')\n",
      "Gradient check passed!\n",
      "Checking gradient for (2, 'B')\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 43.743880, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-177-8dc0e3f02b72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# TODO Implement missing pieces in Trainer.fit function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# You should expect loss to go down every epoch, even if it's slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mloss_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\18888333\\Documents\\HW2\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\18888333\\Documents\\HW2\\model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0m_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\18888333\\Documents\\HW2\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17ec24931d0>]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17ec24533c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 39.131796, Train accuracy: 0.298889, val accuracy: 0.302000\n",
      "Loss: 39.588697, Train accuracy: 0.475667, val accuracy: 0.468000\n",
      "Loss: 32.908086, Train accuracy: 0.477444, val accuracy: 0.481000\n",
      "Loss: 37.058771, Train accuracy: 0.601222, val accuracy: 0.594000\n",
      "Loss: 37.847588, Train accuracy: 0.600889, val accuracy: 0.601000\n",
      "Loss: 34.035028, Train accuracy: 0.548111, val accuracy: 0.531000\n",
      "Loss: 38.098896, Train accuracy: 0.643333, val accuracy: 0.628000\n",
      "Loss: 36.217171, Train accuracy: 0.602444, val accuracy: 0.599000\n",
      "Loss: 28.595014, Train accuracy: 0.592667, val accuracy: 0.587000\n",
      "Loss: 41.493299, Train accuracy: 0.601222, val accuracy: 0.596000\n",
      "Loss: 34.769007, Train accuracy: 0.553333, val accuracy: 0.547000\n",
      "Loss: 27.633100, Train accuracy: 0.596889, val accuracy: 0.586000\n",
      "Loss: 28.788873, Train accuracy: 0.611667, val accuracy: 0.612000\n",
      "Loss: 35.896470, Train accuracy: 0.600222, val accuracy: 0.591000\n",
      "Loss: 35.069728, Train accuracy: 0.611667, val accuracy: 0.589000\n",
      "Loss: 35.815381, Train accuracy: 0.602556, val accuracy: 0.597000\n",
      "Loss: 37.132562, Train accuracy: 0.546444, val accuracy: 0.527000\n",
      "Loss: 37.184411, Train accuracy: 0.582000, val accuracy: 0.584000\n",
      "Loss: 41.528656, Train accuracy: 0.613111, val accuracy: 0.597000\n",
      "Loss: 33.528755, Train accuracy: 0.629222, val accuracy: 0.630000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 46.189893, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 44.903952, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 43.141242, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-95b05c0d8a5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# You should see even better results than before!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mloss_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\18888333\\Documents\\HW2\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\18888333\\Documents\\HW2\\model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFullyConnectedLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0ml2_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml2_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml2_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\18888333\\Documents\\HW2\\layers.py\u001b[0m in \u001b[0;36ml2_regularization\u001b[1;34m(W, reg_strength)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_strength\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mreg_strength\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.542901, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.521914, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.579806, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.453384, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.374456, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.431638, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.245235, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.486756, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.153805, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.181889, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.539431, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.314236, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.079927, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.429651, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.013881, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.268864, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.537703, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.399897, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.060314, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.716208, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.019763, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.354748, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.174352, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.364358, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.943785, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.710020, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.130932, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.347389, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.343111, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 10.156049, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.565511, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.062591, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.065282, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.045532, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.036540, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.779756, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.486296, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.033785, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.291437, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.289042, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.722966, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 10.691439, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.907087, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 10.688185, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 10.969900, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.278951, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.661452, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.624854, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.968698, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.944005, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.293495, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.238639, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.915888, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.226885, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.922142, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.243210, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.190626, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.515385, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.926286, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.902608, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.900467, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.328122, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.477151, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.682747, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.260958, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.688523, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.258855, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 12.073345, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.474571, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.257147, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.892493, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.274046, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.821269, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.254942, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.862149, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.182731, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.408126, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.007424, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.275376, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.994238, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.851679, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.674684, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.782611, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.866222, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.250636, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 10.344780, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 10.842491, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 9.480843, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.839861, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.424141, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.763221, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 8.640872, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.439965, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.437441, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.327287, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.850332, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.247296, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.361012, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.775836, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.825712, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.283793, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.244810, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.874861, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.870480, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.417863, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.243782, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.323201, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.706928, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.804015, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.836852, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.844893, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.289242, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.407414, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.241164, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.813054, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.811881, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 8.860599, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.822400, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.249733, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 12.272237, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.674966, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.238709, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.669736, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.236950, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.806680, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.237327, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 12.858942, Train accuracy: 0.200000, val accuracy: 0.133333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.804170, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.835160, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.803769, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.391211, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.802753, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.801809, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.840406, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.800281, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.232855, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 10.799715, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 10.799448, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 9.771627, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.231352, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.169582, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.797432, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.229881, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.796666, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.852554, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.382509, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.169222, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.815675, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.794030, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.680288, Train accuracy: 0.200000, val accuracy: 0.133333\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-2, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.581979, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.384001, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.223108, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 10.611491, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: 10.372608, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 9.872818, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 8.789478, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 10.325756, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 11.127362, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 12.108627, Train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Loss: 5.647170, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 9.141338, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 6.751879, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 7.682985, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 4.239942, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 4.698452, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.564375, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.233622, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.916069, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.919832, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-2)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-2, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.514770, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 11.512807, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.509375, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.508022, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.501981, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.502096, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.512082, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.506807, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.513214, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.481951, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.492794, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.473896, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.501465, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.456112, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.471148, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.495410, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.507556, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.462769, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.445406, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.439290, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.488659, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.450034, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.505492, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.544921, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.480313, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.389758, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.409349, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.453277, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.452104, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.475322, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.420908, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.444228, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.441205, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.381131, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.468920, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.371540, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.332725, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.498850, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.393311, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.320433, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.389438, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.423259, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.496840, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.380171, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.373791, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.331669, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.497714, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.452757, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.449570, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.358718, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.448470, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.399561, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.351866, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.300280, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.343416, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.392658, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.441519, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.388936, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.385811, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.383112, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.380979, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.322233, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.493431, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.316730, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.490865, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.370489, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.371843, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.306229, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.365385, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.490334, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.424735, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.290071, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.289780, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.286329, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.421025, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.074698, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.422062, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.204949, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.561492, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.417445, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.340407, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.262137, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.264446, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.336606, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.331727, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.256066, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.252457, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.567596, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.243049, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.325154, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.408521, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.488320, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.233043, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.147137, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.230737, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.225558, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.046778, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.221768, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.307687, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.303185, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.031692, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.578222, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.394258, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.111198, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.012166, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.390058, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.581271, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.680210, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.091365, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.287203, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.485611, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 10.981688, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.281669, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.177203, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.278755, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.275898, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.378425, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.274334, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.058106, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.485751, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.375049, Train accuracy: 0.200000, val accuracy: 0.133333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-168-896d536f2e77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMomentumSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mloss_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# TODO find the best hyperparameters to train the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\18888333\\Documents\\HW2\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\18888333\\Documents\\HW2\\model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0m_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\18888333\\Documents\\HW2\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.99\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 5\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hidden_layer_size, reg = reg_strength)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=learning_rates, num_epochs=num_epochs, batch_size=batch_size)\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "# print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c3233981d0>]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAGrCAYAAACIZ9VoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c3232dbc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.537000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
