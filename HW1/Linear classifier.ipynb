{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n",
      "Gradient check passed!\n",
      "[3. 2.]\n",
      "[3. 2.]\n",
      "Gradient check passed!\n",
      "[[3. 2.]\n",
      " [1. 0.]]\n",
      "[[3. 2.]\n",
      " [1. 0.]]\n",
      "[[3. 2.]\n",
      " [1. 0.]]\n",
      "[[3. 2.]\n",
      " [1. 0.]]\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x*x), 3*x*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.551444713932051"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([1, 0, 0]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0.]]\n",
      "[[-1.  0.]]\n",
      "Gradient check passed!\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "[[ 0.  2.  2.  1.]\n",
      " [ 2.  1.  2. -1.]\n",
      " [ 2.  1.  1.  0.]]\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 2\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "`\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 24922.369572\n",
      "Epoch 1, loss: 27568.257028\n",
      "Epoch 2, loss: 25915.370534\n",
      "Epoch 3, loss: 25485.690955\n",
      "Epoch 4, loss: 26419.512496\n",
      "Epoch 5, loss: 26549.373371\n",
      "Epoch 6, loss: 25401.388208\n",
      "Epoch 7, loss: 27931.257945\n",
      "Epoch 8, loss: 25479.705492\n",
      "Epoch 9, loss: 27469.742426\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20c8f65d710>]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQ4Gg8FguIr/H3hVbJYbo9maAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c8f5a00b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.133\n",
      "Epoch 0, loss: 25978.411762\n",
      "Epoch 1, loss: 27381.633479\n",
      "Epoch 2, loss: 26266.139362\n",
      "Epoch 3, loss: 26511.701239\n",
      "Epoch 4, loss: 26470.962245\n",
      "Epoch 5, loss: 25424.576244\n",
      "Epoch 6, loss: 28634.990773\n",
      "Epoch 7, loss: 26213.566567\n",
      "Epoch 8, loss: 27093.455477\n",
      "Epoch 9, loss: 26834.162172\n",
      "Epoch 10, loss: 26454.338859\n",
      "Epoch 11, loss: 25437.447439\n",
      "Epoch 12, loss: 26607.349671\n",
      "Epoch 13, loss: 26649.102200\n",
      "Epoch 14, loss: 26118.862303\n",
      "Epoch 15, loss: 25956.558123\n",
      "Epoch 16, loss: 25944.224290\n",
      "Epoch 17, loss: 26986.347475\n",
      "Epoch 18, loss: 26939.865937\n",
      "Epoch 19, loss: 26317.581044\n",
      "Epoch 20, loss: 26442.674224\n",
      "Epoch 21, loss: 25094.637598\n",
      "Epoch 22, loss: 26185.744503\n",
      "Epoch 23, loss: 26573.473301\n",
      "Epoch 24, loss: 25448.345896\n",
      "Epoch 25, loss: 26575.668666\n",
      "Epoch 26, loss: 25837.265487\n",
      "Epoch 27, loss: 26470.927110\n",
      "Epoch 28, loss: 25756.586727\n",
      "Epoch 29, loss: 27152.838015\n",
      "Epoch 30, loss: 26472.760036\n",
      "Epoch 31, loss: 26197.815815\n",
      "Epoch 32, loss: 28035.603736\n",
      "Epoch 33, loss: 26864.896440\n",
      "Epoch 34, loss: 27059.470268\n",
      "Epoch 35, loss: 26668.453835\n",
      "Epoch 36, loss: 25599.312882\n",
      "Epoch 37, loss: 28042.939925\n",
      "Epoch 38, loss: 27235.638249\n",
      "Epoch 39, loss: 27603.908735\n",
      "Epoch 40, loss: 27896.105683\n",
      "Epoch 41, loss: 25175.184326\n",
      "Epoch 42, loss: 25332.173288\n",
      "Epoch 43, loss: 27534.174711\n",
      "Epoch 44, loss: 25646.409751\n",
      "Epoch 45, loss: 27776.715180\n",
      "Epoch 46, loss: 26596.668786\n",
      "Epoch 47, loss: 24943.515197\n",
      "Epoch 48, loss: 26957.041807\n",
      "Epoch 49, loss: 25432.342064\n",
      "Epoch 50, loss: 26648.354429\n",
      "Epoch 51, loss: 27537.075086\n",
      "Epoch 52, loss: 26689.870277\n",
      "Epoch 53, loss: 26533.285838\n",
      "Epoch 54, loss: 26751.511458\n",
      "Epoch 55, loss: 27899.700836\n",
      "Epoch 56, loss: 25920.154357\n",
      "Epoch 57, loss: 26411.813476\n",
      "Epoch 58, loss: 26659.872885\n",
      "Epoch 59, loss: 27331.894374\n",
      "Epoch 60, loss: 25478.437965\n",
      "Epoch 61, loss: 26835.040954\n",
      "Epoch 62, loss: 27918.890752\n",
      "Epoch 63, loss: 27096.641818\n",
      "Epoch 64, loss: 25781.645393\n",
      "Epoch 65, loss: 27530.463524\n",
      "Epoch 66, loss: 25963.739625\n",
      "Epoch 67, loss: 27271.888500\n",
      "Epoch 68, loss: 24947.091323\n",
      "Epoch 69, loss: 27269.435203\n",
      "Epoch 70, loss: 26461.755668\n",
      "Epoch 71, loss: 25738.323074\n",
      "Epoch 72, loss: 25017.118691\n",
      "Epoch 73, loss: 27679.433876\n",
      "Epoch 74, loss: 26918.840966\n",
      "Epoch 75, loss: 26304.299282\n",
      "Epoch 76, loss: 26143.623534\n",
      "Epoch 77, loss: 26687.570754\n",
      "Epoch 78, loss: 26715.569883\n",
      "Epoch 79, loss: 26576.655707\n",
      "Epoch 80, loss: 27987.290944\n",
      "Epoch 81, loss: 25142.492370\n",
      "Epoch 82, loss: 27386.130430\n",
      "Epoch 83, loss: 26477.432436\n",
      "Epoch 84, loss: 27000.318956\n",
      "Epoch 85, loss: 27631.791430\n",
      "Epoch 86, loss: 25686.782254\n",
      "Epoch 87, loss: 26128.404900\n",
      "Epoch 88, loss: 25991.257766\n",
      "Epoch 89, loss: 26720.040348\n",
      "Epoch 90, loss: 26027.538164\n",
      "Epoch 91, loss: 26417.757097\n",
      "Epoch 92, loss: 26675.741043\n",
      "Epoch 93, loss: 28436.954790\n",
      "Epoch 94, loss: 25939.162951\n",
      "Epoch 95, loss: 26637.219538\n",
      "Epoch 96, loss: 27470.167977\n",
      "Epoch 97, loss: 26352.759917\n",
      "Epoch 98, loss: 26251.536823\n",
      "Epoch 99, loss: 26191.953189\n",
      "Accuracy after training for 100 epochs:  0.16\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 23052.014226\n",
      "Epoch 1, loss: 20490.188903\n",
      "Epoch 2, loss: 21865.312162\n",
      "Epoch 3, loss: 20713.859037\n",
      "Epoch 4, loss: 21237.579629\n",
      "Epoch 5, loss: 22227.257348\n",
      "Epoch 6, loss: 20183.110101\n",
      "Epoch 7, loss: 20922.269372\n",
      "Epoch 8, loss: 21888.141109\n",
      "Epoch 9, loss: 19968.801980\n",
      "Epoch 10, loss: 21279.017265\n",
      "Epoch 11, loss: 21500.608902\n",
      "Epoch 12, loss: 20735.518011\n",
      "Epoch 13, loss: 20278.927904\n",
      "Epoch 14, loss: 20314.923282\n",
      "Epoch 15, loss: 20040.293554\n",
      "Epoch 16, loss: 21453.174382\n",
      "Epoch 17, loss: 20126.207701\n",
      "Epoch 18, loss: 19926.232867\n",
      "Epoch 19, loss: 21079.937196\n",
      "Epoch 20, loss: 18620.887066\n",
      "Epoch 21, loss: 21529.372866\n",
      "Epoch 22, loss: 20047.843790\n",
      "Epoch 23, loss: 19515.034390\n",
      "Epoch 24, loss: 18834.875902\n",
      "Epoch 25, loss: 21704.734539\n",
      "Epoch 26, loss: 20921.484789\n",
      "Epoch 27, loss: 20089.402513\n",
      "Epoch 28, loss: 21237.589217\n",
      "Epoch 29, loss: 21013.232230\n",
      "Epoch 30, loss: 20724.044198\n",
      "Epoch 31, loss: 20964.822208\n",
      "Epoch 32, loss: 20507.890050\n",
      "Epoch 33, loss: 20019.477063\n",
      "Epoch 34, loss: 20090.375842\n",
      "Epoch 35, loss: 22019.483736\n",
      "Epoch 36, loss: 22376.394650\n",
      "Epoch 37, loss: 21328.253215\n",
      "Epoch 38, loss: 20532.556747\n",
      "Epoch 39, loss: 21159.950884\n",
      "Epoch 40, loss: 20847.016549\n",
      "Epoch 41, loss: 20861.432295\n",
      "Epoch 42, loss: 19896.287883\n",
      "Epoch 43, loss: 20924.536008\n",
      "Epoch 44, loss: 19907.726543\n",
      "Epoch 45, loss: 19848.836304\n",
      "Epoch 46, loss: 21644.721535\n",
      "Epoch 47, loss: 19493.234386\n",
      "Epoch 48, loss: 20386.725368\n",
      "Epoch 49, loss: 20814.416321\n",
      "Epoch 50, loss: 20179.247098\n",
      "Epoch 51, loss: 20612.872591\n",
      "Epoch 52, loss: 20369.222321\n",
      "Epoch 53, loss: 22127.282512\n",
      "Epoch 54, loss: 20448.676851\n",
      "Epoch 55, loss: 20331.122552\n",
      "Epoch 56, loss: 20017.298845\n",
      "Epoch 57, loss: 20743.475059\n",
      "Epoch 58, loss: 20177.510985\n",
      "Epoch 59, loss: 20116.221681\n",
      "Epoch 60, loss: 21396.955530\n",
      "Epoch 61, loss: 20342.030904\n",
      "Epoch 62, loss: 20796.911209\n",
      "Epoch 63, loss: 20563.880899\n",
      "Epoch 64, loss: 19879.541678\n",
      "Epoch 65, loss: 21683.558228\n",
      "Epoch 66, loss: 20413.790982\n",
      "Epoch 67, loss: 20207.399990\n",
      "Epoch 68, loss: 20256.822179\n",
      "Epoch 69, loss: 21001.368225\n",
      "Epoch 70, loss: 20445.356305\n",
      "Epoch 71, loss: 20128.723316\n",
      "Epoch 72, loss: 19270.547995\n",
      "Epoch 73, loss: 20890.285643\n",
      "Epoch 74, loss: 19390.080720\n",
      "Epoch 75, loss: 21196.414673\n",
      "Epoch 76, loss: 19985.284130\n",
      "Epoch 77, loss: 20307.214395\n",
      "Epoch 78, loss: 19919.496174\n",
      "Epoch 79, loss: 20060.265305\n",
      "Epoch 80, loss: 19857.507729\n",
      "Epoch 81, loss: 21725.000536\n",
      "Epoch 82, loss: 21666.255213\n",
      "Epoch 83, loss: 20644.365462\n",
      "Epoch 84, loss: 20039.318734\n",
      "Epoch 85, loss: 21085.302890\n",
      "Epoch 86, loss: 20837.431766\n",
      "Epoch 87, loss: 20357.543251\n",
      "Epoch 88, loss: 20840.760755\n",
      "Epoch 89, loss: 19979.064830\n",
      "Epoch 90, loss: 20261.635518\n",
      "Epoch 91, loss: 20813.440861\n",
      "Epoch 92, loss: 20388.466103\n",
      "Epoch 93, loss: 20178.657248\n",
      "Epoch 94, loss: 21358.133076\n",
      "Epoch 95, loss: 19975.533229\n",
      "Epoch 96, loss: 20693.424831\n",
      "Epoch 97, loss: 21537.360715\n",
      "Epoch 98, loss: 19552.590123\n",
      "Epoch 99, loss: 20719.970210\n",
      "Epoch 100, loss: 21180.336227\n",
      "Epoch 101, loss: 19631.299149\n",
      "Epoch 102, loss: 21025.081679\n",
      "Epoch 103, loss: 20584.625070\n",
      "Epoch 104, loss: 20036.514056\n",
      "Epoch 105, loss: 20261.719056\n",
      "Epoch 106, loss: 18367.751330\n",
      "Epoch 107, loss: 21231.237536\n",
      "Epoch 108, loss: 20689.427459\n",
      "Epoch 109, loss: 20459.637323\n",
      "Epoch 110, loss: 19979.174989\n",
      "Epoch 111, loss: 20175.232400\n",
      "Epoch 112, loss: 21090.108759\n",
      "Epoch 113, loss: 20121.619129\n",
      "Epoch 114, loss: 20066.112579\n",
      "Epoch 115, loss: 20468.539830\n",
      "Epoch 116, loss: 20440.689055\n",
      "Epoch 117, loss: 20197.511335\n",
      "Epoch 118, loss: 21193.117949\n",
      "Epoch 119, loss: 19954.164763\n",
      "Epoch 120, loss: 20879.638114\n",
      "Epoch 121, loss: 20403.410739\n",
      "Epoch 122, loss: 19866.666770\n",
      "Epoch 123, loss: 20161.732318\n",
      "Epoch 124, loss: 19659.220135\n",
      "Epoch 125, loss: 21023.221641\n",
      "Epoch 126, loss: 19990.762001\n",
      "Epoch 127, loss: 20102.208206\n",
      "Epoch 128, loss: 19753.796229\n",
      "Epoch 129, loss: 20973.835268\n",
      "Epoch 130, loss: 19632.710592\n",
      "Epoch 131, loss: 21036.962428\n",
      "Epoch 132, loss: 19271.036542\n",
      "Epoch 133, loss: 20780.331369\n",
      "Epoch 134, loss: 19229.371249\n",
      "Epoch 135, loss: 20899.204710\n",
      "Epoch 136, loss: 20306.247300\n",
      "Epoch 137, loss: 19016.350878\n",
      "Epoch 138, loss: 20969.105275\n",
      "Epoch 139, loss: 20134.052501\n",
      "Epoch 140, loss: 21715.890001\n",
      "Epoch 141, loss: 19364.166801\n",
      "Epoch 142, loss: 20506.121893\n",
      "Epoch 143, loss: 19881.645451\n",
      "Epoch 144, loss: 21392.868388\n",
      "Epoch 145, loss: 20287.370016\n",
      "Epoch 146, loss: 20249.716112\n",
      "Epoch 147, loss: 20113.730293\n",
      "Epoch 148, loss: 20164.203866\n",
      "Epoch 149, loss: 20356.924825\n",
      "Epoch 150, loss: 19340.988846\n",
      "Epoch 151, loss: 21844.852914\n",
      "Epoch 152, loss: 19820.523181\n",
      "Epoch 153, loss: 19784.503920\n",
      "Epoch 154, loss: 20642.093708\n",
      "Epoch 155, loss: 20776.359007\n",
      "Epoch 156, loss: 20879.245152\n",
      "Epoch 157, loss: 20335.706988\n",
      "Epoch 158, loss: 19045.339075\n",
      "Epoch 159, loss: 21367.460186\n",
      "Epoch 160, loss: 19344.370400\n",
      "Epoch 161, loss: 20488.902393\n",
      "Epoch 162, loss: 20000.262151\n",
      "Epoch 163, loss: 20611.772882\n",
      "Epoch 164, loss: 20101.520112\n",
      "Epoch 165, loss: 21094.032107\n",
      "Epoch 166, loss: 20166.171735\n",
      "Epoch 167, loss: 20321.269234\n",
      "Epoch 168, loss: 19012.077748\n",
      "Epoch 169, loss: 20873.885296\n",
      "Epoch 170, loss: 20974.537973\n",
      "Epoch 171, loss: 20042.490800\n",
      "Epoch 172, loss: 19993.325690\n",
      "Epoch 173, loss: 19144.100520\n",
      "Epoch 174, loss: 20861.966400\n",
      "Epoch 175, loss: 19881.698459\n",
      "Epoch 176, loss: 21139.835326\n",
      "Epoch 177, loss: 18406.571344\n",
      "Epoch 178, loss: 21062.801263\n",
      "Epoch 179, loss: 19868.166759\n",
      "Epoch 180, loss: 20952.826799\n",
      "Epoch 181, loss: 18760.301710\n",
      "Epoch 182, loss: 20389.108160\n",
      "Epoch 183, loss: 19398.753894\n",
      "Epoch 184, loss: 21148.591312\n",
      "Epoch 185, loss: 20686.639237\n",
      "Epoch 186, loss: 21088.254937\n",
      "Epoch 187, loss: 20205.424769\n",
      "Epoch 188, loss: 20261.558860\n",
      "Epoch 189, loss: 19760.222377\n",
      "Epoch 190, loss: 19944.281463\n",
      "Epoch 191, loss: 19467.005647\n",
      "Epoch 192, loss: 21016.878104\n",
      "Epoch 193, loss: 20689.772377\n",
      "Epoch 194, loss: 19715.100486\n",
      "Epoch 195, loss: 21176.850132\n",
      "Epoch 196, loss: 20373.342994\n",
      "Epoch 197, loss: 20129.094739\n",
      "Epoch 198, loss: 19777.369522\n",
      "Epoch 199, loss: 20907.082260\n",
      "Epoch 0, loss: 19656.417412\n",
      "Epoch 1, loss: 20978.657948\n",
      "Epoch 2, loss: 20089.472480\n",
      "Epoch 3, loss: 20601.080130\n",
      "Epoch 4, loss: 20181.307819\n",
      "Epoch 5, loss: 20388.183238\n",
      "Epoch 6, loss: 19870.751914\n",
      "Epoch 7, loss: 19775.190465\n",
      "Epoch 8, loss: 20274.773953\n",
      "Epoch 9, loss: 20582.826562\n",
      "Epoch 10, loss: 19650.896076\n",
      "Epoch 11, loss: 20766.524556\n",
      "Epoch 12, loss: 19168.076363\n",
      "Epoch 13, loss: 21280.245435\n",
      "Epoch 14, loss: 18733.701548\n",
      "Epoch 15, loss: 20720.315472\n",
      "Epoch 16, loss: 19564.226426\n",
      "Epoch 17, loss: 20670.338477\n",
      "Epoch 18, loss: 20179.788319\n",
      "Epoch 19, loss: 21130.441323\n",
      "Epoch 20, loss: 20221.951271\n",
      "Epoch 21, loss: 20328.084027\n",
      "Epoch 22, loss: 20206.909287\n",
      "Epoch 23, loss: 18115.192540\n",
      "Epoch 24, loss: 20882.675547\n",
      "Epoch 25, loss: 20329.515479\n",
      "Epoch 26, loss: 19260.996573\n",
      "Epoch 27, loss: 20578.354482\n",
      "Epoch 28, loss: 20019.927428\n",
      "Epoch 29, loss: 20489.270034\n",
      "Epoch 30, loss: 20312.567571\n",
      "Epoch 31, loss: 20109.929985\n",
      "Epoch 32, loss: 20829.233749\n",
      "Epoch 33, loss: 20363.476295\n",
      "Epoch 34, loss: 20123.595219\n",
      "Epoch 35, loss: 19082.960711\n",
      "Epoch 36, loss: 21546.713945\n",
      "Epoch 37, loss: 19683.918668\n",
      "Epoch 38, loss: 21368.802417\n",
      "Epoch 39, loss: 19862.409430\n",
      "Epoch 40, loss: 20683.787246\n",
      "Epoch 41, loss: 20398.398244\n",
      "Epoch 42, loss: 21050.113235\n",
      "Epoch 43, loss: 19745.354709\n",
      "Epoch 44, loss: 19905.246758\n",
      "Epoch 45, loss: 20181.172655\n",
      "Epoch 46, loss: 20104.522835\n",
      "Epoch 47, loss: 20708.398579\n",
      "Epoch 48, loss: 20019.938071\n",
      "Epoch 49, loss: 19552.358406\n",
      "Epoch 50, loss: 20811.058521\n",
      "Epoch 51, loss: 19464.076678\n",
      "Epoch 52, loss: 19679.318630\n",
      "Epoch 53, loss: 21611.037751\n",
      "Epoch 54, loss: 19521.788686\n",
      "Epoch 55, loss: 19821.870708\n",
      "Epoch 56, loss: 20139.923336\n",
      "Epoch 57, loss: 20515.545269\n",
      "Epoch 58, loss: 19140.815740\n",
      "Epoch 59, loss: 19909.421593\n",
      "Epoch 60, loss: 19218.048965\n",
      "Epoch 61, loss: 20712.173484\n",
      "Epoch 62, loss: 19500.956380\n",
      "Epoch 63, loss: 20111.736539\n",
      "Epoch 64, loss: 19827.461976\n",
      "Epoch 65, loss: 19568.608795\n",
      "Epoch 66, loss: 20751.718465\n",
      "Epoch 67, loss: 20252.885303\n",
      "Epoch 68, loss: 20238.612927\n",
      "Epoch 69, loss: 20075.966356\n",
      "Epoch 70, loss: 20821.100783\n",
      "Epoch 71, loss: 20475.686558\n",
      "Epoch 72, loss: 20294.240354\n",
      "Epoch 73, loss: 21970.571314\n",
      "Epoch 74, loss: 19556.923825\n",
      "Epoch 75, loss: 20303.535126\n",
      "Epoch 76, loss: 20453.512612\n",
      "Epoch 77, loss: 20453.692819\n",
      "Epoch 78, loss: 20633.262629\n",
      "Epoch 79, loss: 20648.839770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, loss: 19343.302752\n",
      "Epoch 81, loss: 20561.063910\n",
      "Epoch 82, loss: 20112.521290\n",
      "Epoch 83, loss: 20126.519581\n",
      "Epoch 84, loss: 19995.031750\n",
      "Epoch 85, loss: 20048.822051\n",
      "Epoch 86, loss: 20961.311141\n",
      "Epoch 87, loss: 20192.550032\n",
      "Epoch 88, loss: 20376.786759\n",
      "Epoch 89, loss: 21883.025866\n",
      "Epoch 90, loss: 18929.984960\n",
      "Epoch 91, loss: 19537.777207\n",
      "Epoch 92, loss: 20209.803771\n",
      "Epoch 93, loss: 19850.770847\n",
      "Epoch 94, loss: 20806.059704\n",
      "Epoch 95, loss: 19853.331797\n",
      "Epoch 96, loss: 19177.782759\n",
      "Epoch 97, loss: 19255.504450\n",
      "Epoch 98, loss: 19879.700106\n",
      "Epoch 99, loss: 19988.219608\n",
      "Epoch 100, loss: 20078.978781\n",
      "Epoch 101, loss: 21144.801134\n",
      "Epoch 102, loss: 20299.741113\n",
      "Epoch 103, loss: 20659.120497\n",
      "Epoch 104, loss: 19984.083475\n",
      "Epoch 105, loss: 21350.585428\n",
      "Epoch 106, loss: 19497.423760\n",
      "Epoch 107, loss: 20055.151832\n",
      "Epoch 108, loss: 20537.070440\n",
      "Epoch 109, loss: 19152.463097\n",
      "Epoch 110, loss: 21520.021497\n",
      "Epoch 111, loss: 20723.020548\n",
      "Epoch 112, loss: 20507.144309\n",
      "Epoch 113, loss: 20032.353623\n",
      "Epoch 114, loss: 20573.997803\n",
      "Epoch 115, loss: 20289.464499\n",
      "Epoch 116, loss: 19778.056553\n",
      "Epoch 117, loss: 19557.449400\n",
      "Epoch 118, loss: 19646.101475\n",
      "Epoch 119, loss: 20822.742799\n",
      "Epoch 120, loss: 19872.184496\n",
      "Epoch 121, loss: 20262.716598\n",
      "Epoch 122, loss: 19715.836794\n",
      "Epoch 123, loss: 20442.221788\n",
      "Epoch 124, loss: 19836.997575\n",
      "Epoch 125, loss: 20995.854051\n",
      "Epoch 126, loss: 19541.537354\n",
      "Epoch 127, loss: 20378.723475\n",
      "Epoch 128, loss: 20090.511638\n",
      "Epoch 129, loss: 20923.460815\n",
      "Epoch 130, loss: 19579.682850\n",
      "Epoch 131, loss: 19889.789827\n",
      "Epoch 132, loss: 20187.098073\n",
      "Epoch 133, loss: 20990.089854\n",
      "Epoch 134, loss: 20813.117225\n",
      "Epoch 135, loss: 19912.898658\n",
      "Epoch 136, loss: 17728.079327\n",
      "Epoch 137, loss: 21354.255595\n",
      "Epoch 138, loss: 19792.207443\n",
      "Epoch 139, loss: 18949.026737\n",
      "Epoch 140, loss: 20513.265735\n",
      "Epoch 141, loss: 19736.639955\n",
      "Epoch 142, loss: 18890.814437\n",
      "Epoch 143, loss: 20242.525245\n",
      "Epoch 144, loss: 18978.880947\n",
      "Epoch 145, loss: 19852.379890\n",
      "Epoch 146, loss: 19077.053436\n",
      "Epoch 147, loss: 20040.975977\n",
      "Epoch 148, loss: 20640.090656\n",
      "Epoch 149, loss: 21487.226783\n",
      "Epoch 150, loss: 19827.045048\n",
      "Epoch 151, loss: 20473.373242\n",
      "Epoch 152, loss: 19911.535997\n",
      "Epoch 153, loss: 19691.925634\n",
      "Epoch 154, loss: 19038.354973\n",
      "Epoch 155, loss: 20141.409701\n",
      "Epoch 156, loss: 19901.360860\n",
      "Epoch 157, loss: 19012.396801\n",
      "Epoch 158, loss: 20070.999664\n",
      "Epoch 159, loss: 19925.661563\n",
      "Epoch 160, loss: 20230.424231\n",
      "Epoch 161, loss: 20469.708162\n",
      "Epoch 162, loss: 18101.115324\n",
      "Epoch 163, loss: 21186.119331\n",
      "Epoch 164, loss: 18145.445205\n",
      "Epoch 165, loss: 20867.779817\n",
      "Epoch 166, loss: 20077.059105\n",
      "Epoch 167, loss: 20171.235917\n",
      "Epoch 168, loss: 19077.290376\n",
      "Epoch 169, loss: 19814.742903\n",
      "Epoch 170, loss: 20562.756503\n",
      "Epoch 171, loss: 18601.772357\n",
      "Epoch 172, loss: 19770.728969\n",
      "Epoch 173, loss: 19510.006770\n",
      "Epoch 174, loss: 20241.972096\n",
      "Epoch 175, loss: 20014.263939\n",
      "Epoch 176, loss: 20459.105774\n",
      "Epoch 177, loss: 19657.663554\n",
      "Epoch 178, loss: 20343.956511\n",
      "Epoch 179, loss: 20504.228187\n",
      "Epoch 180, loss: 19360.039161\n",
      "Epoch 181, loss: 19493.600821\n",
      "Epoch 182, loss: 20223.282978\n",
      "Epoch 183, loss: 19623.669485\n",
      "Epoch 184, loss: 20923.676312\n",
      "Epoch 185, loss: 20034.588792\n",
      "Epoch 186, loss: 19459.712962\n",
      "Epoch 187, loss: 21046.237317\n",
      "Epoch 188, loss: 20030.079616\n",
      "Epoch 189, loss: 18972.199859\n",
      "Epoch 190, loss: 19458.075264\n",
      "Epoch 191, loss: 21235.677891\n",
      "Epoch 192, loss: 19836.299154\n",
      "Epoch 193, loss: 18302.252187\n",
      "Epoch 194, loss: 20765.659126\n",
      "Epoch 195, loss: 19706.094353\n",
      "Epoch 196, loss: 19883.668682\n",
      "Epoch 197, loss: 18664.545644\n",
      "Epoch 198, loss: 20383.350133\n",
      "Epoch 199, loss: 19828.417963\n",
      "Epoch 0, loss: 19917.438450\n",
      "Epoch 1, loss: 19194.091385\n",
      "Epoch 2, loss: 20763.339029\n",
      "Epoch 3, loss: 18796.004361\n",
      "Epoch 4, loss: 21397.490386\n",
      "Epoch 5, loss: 18633.134394\n",
      "Epoch 6, loss: 19661.132538\n",
      "Epoch 7, loss: 20111.799174\n",
      "Epoch 8, loss: 20426.790999\n",
      "Epoch 9, loss: 18684.656767\n",
      "Epoch 10, loss: 20111.791404\n",
      "Epoch 11, loss: 20430.366391\n",
      "Epoch 12, loss: 18996.189044\n",
      "Epoch 13, loss: 20315.220839\n",
      "Epoch 14, loss: 19675.989209\n",
      "Epoch 15, loss: 18653.410717\n",
      "Epoch 16, loss: 19979.096976\n",
      "Epoch 17, loss: 18484.384947\n",
      "Epoch 18, loss: 20872.261986\n",
      "Epoch 19, loss: 20554.337170\n",
      "Epoch 20, loss: 20200.178042\n",
      "Epoch 21, loss: 20891.741060\n",
      "Epoch 22, loss: 19712.219730\n",
      "Epoch 23, loss: 20726.008665\n",
      "Epoch 24, loss: 19413.144291\n",
      "Epoch 25, loss: 20816.144243\n",
      "Epoch 26, loss: 19569.779165\n",
      "Epoch 27, loss: 20856.538164\n",
      "Epoch 28, loss: 19178.768253\n",
      "Epoch 29, loss: 19419.337483\n",
      "Epoch 30, loss: 20297.337208\n",
      "Epoch 31, loss: 19715.904817\n",
      "Epoch 32, loss: 20621.339219\n",
      "Epoch 33, loss: 19999.943571\n",
      "Epoch 34, loss: 19844.086483\n",
      "Epoch 35, loss: 19188.271487\n",
      "Epoch 36, loss: 20548.365417\n",
      "Epoch 37, loss: 19122.618779\n",
      "Epoch 38, loss: 21125.373955\n",
      "Epoch 39, loss: 19894.644159\n",
      "Epoch 40, loss: 20452.722357\n",
      "Epoch 41, loss: 19281.530779\n",
      "Epoch 42, loss: 19154.241612\n",
      "Epoch 43, loss: 20312.845832\n",
      "Epoch 44, loss: 19134.020895\n",
      "Epoch 45, loss: 20065.336539\n",
      "Epoch 46, loss: 19086.872418\n",
      "Epoch 47, loss: 19352.753021\n",
      "Epoch 48, loss: 20482.616601\n",
      "Epoch 49, loss: 20500.075316\n",
      "Epoch 50, loss: 19829.050991\n",
      "Epoch 51, loss: 20058.184523\n",
      "Epoch 52, loss: 20015.459011\n",
      "Epoch 53, loss: 19199.622939\n",
      "Epoch 54, loss: 19552.069351\n",
      "Epoch 55, loss: 19966.352148\n",
      "Epoch 56, loss: 20259.063318\n",
      "Epoch 57, loss: 18906.842690\n",
      "Epoch 58, loss: 20084.629201\n",
      "Epoch 59, loss: 18879.236665\n",
      "Epoch 60, loss: 20716.901120\n",
      "Epoch 61, loss: 19757.230827\n",
      "Epoch 62, loss: 19791.166484\n",
      "Epoch 63, loss: 19945.591371\n",
      "Epoch 64, loss: 20519.096513\n",
      "Epoch 65, loss: 19684.226051\n",
      "Epoch 66, loss: 19328.771389\n",
      "Epoch 67, loss: 20222.233211\n",
      "Epoch 68, loss: 19045.599922\n",
      "Epoch 69, loss: 19862.853137\n",
      "Epoch 70, loss: 19438.948850\n",
      "Epoch 71, loss: 21392.485136\n",
      "Epoch 72, loss: 18104.473579\n",
      "Epoch 73, loss: 20657.427995\n",
      "Epoch 74, loss: 19262.758643\n",
      "Epoch 75, loss: 19403.909880\n",
      "Epoch 76, loss: 20324.305440\n",
      "Epoch 77, loss: 19485.058662\n",
      "Epoch 78, loss: 20165.282881\n",
      "Epoch 79, loss: 18505.281463\n",
      "Epoch 80, loss: 20896.188386\n",
      "Epoch 81, loss: 20507.256673\n",
      "Epoch 82, loss: 20032.118786\n",
      "Epoch 83, loss: 19697.557959\n",
      "Epoch 84, loss: 20061.060245\n",
      "Epoch 85, loss: 19079.308271\n",
      "Epoch 86, loss: 20185.623198\n",
      "Epoch 87, loss: 20308.073754\n",
      "Epoch 88, loss: 20712.132011\n",
      "Epoch 89, loss: 19366.514803\n",
      "Epoch 90, loss: 19778.164581\n",
      "Epoch 91, loss: 19271.391786\n",
      "Epoch 92, loss: 20106.883281\n",
      "Epoch 93, loss: 20041.159800\n",
      "Epoch 94, loss: 18904.795490\n",
      "Epoch 95, loss: 20489.350477\n",
      "Epoch 96, loss: 18547.668485\n",
      "Epoch 97, loss: 19101.348071\n",
      "Epoch 98, loss: 19540.652574\n",
      "Epoch 99, loss: 20446.574588\n",
      "Epoch 100, loss: 19521.749907\n",
      "Epoch 101, loss: 19703.483810\n",
      "Epoch 102, loss: 19630.904196\n",
      "Epoch 103, loss: 19524.259270\n",
      "Epoch 104, loss: 21077.998381\n",
      "Epoch 105, loss: 19233.632125\n",
      "Epoch 106, loss: 19002.190182\n",
      "Epoch 107, loss: 19505.171897\n",
      "Epoch 108, loss: 21541.627580\n",
      "Epoch 109, loss: 20787.135769\n",
      "Epoch 110, loss: 18063.073464\n",
      "Epoch 111, loss: 19383.038230\n",
      "Epoch 112, loss: 20491.700642\n",
      "Epoch 113, loss: 19830.823149\n",
      "Epoch 114, loss: 20240.002832\n",
      "Epoch 115, loss: 19219.792230\n",
      "Epoch 116, loss: 19839.524786\n",
      "Epoch 117, loss: 19518.637164\n",
      "Epoch 118, loss: 20198.561472\n",
      "Epoch 119, loss: 20349.115270\n",
      "Epoch 120, loss: 18567.731690\n",
      "Epoch 121, loss: 20091.596668\n",
      "Epoch 122, loss: 20122.765120\n",
      "Epoch 123, loss: 18715.928191\n",
      "Epoch 124, loss: 19985.952316\n",
      "Epoch 125, loss: 19086.916108\n",
      "Epoch 126, loss: 20259.943803\n",
      "Epoch 127, loss: 20800.817456\n",
      "Epoch 128, loss: 18891.151172\n",
      "Epoch 129, loss: 19944.574297\n",
      "Epoch 130, loss: 19326.638294\n",
      "Epoch 131, loss: 19847.912682\n",
      "Epoch 132, loss: 20552.430715\n",
      "Epoch 133, loss: 19420.230002\n",
      "Epoch 134, loss: 19952.072205\n",
      "Epoch 135, loss: 19411.683662\n",
      "Epoch 136, loss: 19781.939690\n",
      "Epoch 137, loss: 21535.424080\n",
      "Epoch 138, loss: 20089.288165\n",
      "Epoch 139, loss: 20512.245360\n",
      "Epoch 140, loss: 18367.052636\n",
      "Epoch 141, loss: 19986.029890\n",
      "Epoch 142, loss: 20205.533787\n",
      "Epoch 143, loss: 20185.482349\n",
      "Epoch 144, loss: 19429.589863\n",
      "Epoch 145, loss: 19687.601028\n",
      "Epoch 146, loss: 19014.768032\n",
      "Epoch 147, loss: 19095.256591\n",
      "Epoch 148, loss: 19781.036410\n",
      "Epoch 149, loss: 20164.007777\n",
      "Epoch 150, loss: 18961.125894\n",
      "Epoch 151, loss: 18879.416932\n",
      "Epoch 152, loss: 20404.714195\n",
      "Epoch 153, loss: 20983.852685\n",
      "Epoch 154, loss: 20400.031159\n",
      "Epoch 155, loss: 18963.102682\n",
      "Epoch 156, loss: 20616.117751\n",
      "Epoch 157, loss: 19785.733716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158, loss: 20577.354144\n",
      "Epoch 159, loss: 19804.863549\n",
      "Epoch 160, loss: 19340.194643\n",
      "Epoch 161, loss: 18914.925391\n",
      "Epoch 162, loss: 19088.964533\n",
      "Epoch 163, loss: 22233.875207\n",
      "Epoch 164, loss: 19093.386423\n",
      "Epoch 165, loss: 20309.660825\n",
      "Epoch 166, loss: 19924.365366\n",
      "Epoch 167, loss: 19754.558140\n",
      "Epoch 168, loss: 19077.867890\n",
      "Epoch 169, loss: 19490.499083\n",
      "Epoch 170, loss: 19933.142048\n",
      "Epoch 171, loss: 19594.178914\n",
      "Epoch 172, loss: 19055.576036\n",
      "Epoch 173, loss: 20053.612297\n",
      "Epoch 174, loss: 19992.243747\n",
      "Epoch 175, loss: 20299.471251\n",
      "Epoch 176, loss: 20393.005116\n",
      "Epoch 177, loss: 19080.644252\n",
      "Epoch 178, loss: 19135.839432\n",
      "Epoch 179, loss: 19702.584562\n",
      "Epoch 180, loss: 18731.090820\n",
      "Epoch 181, loss: 20081.762331\n",
      "Epoch 182, loss: 20020.449657\n",
      "Epoch 183, loss: 19324.794281\n",
      "Epoch 184, loss: 20722.692710\n",
      "Epoch 185, loss: 20003.350826\n",
      "Epoch 186, loss: 18793.295079\n",
      "Epoch 187, loss: 20542.844286\n",
      "Epoch 188, loss: 19097.544010\n",
      "Epoch 189, loss: 20028.231569\n",
      "Epoch 190, loss: 20158.184265\n",
      "Epoch 191, loss: 19180.851843\n",
      "Epoch 192, loss: 20807.655354\n",
      "Epoch 193, loss: 20047.732307\n",
      "Epoch 194, loss: 18862.745035\n",
      "Epoch 195, loss: 20173.686414\n",
      "Epoch 196, loss: 20769.513651\n",
      "Epoch 197, loss: 19392.202352\n",
      "Epoch 198, loss: 18752.169959\n",
      "Epoch 199, loss: 19915.149487\n",
      "Epoch 0, loss: 16525.310313\n",
      "Epoch 1, loss: 16127.042360\n",
      "Epoch 2, loss: 16092.172712\n",
      "Epoch 3, loss: 16063.050234\n",
      "Epoch 4, loss: 16043.052378\n",
      "Epoch 5, loss: 16024.886895\n",
      "Epoch 6, loss: 16011.223370\n",
      "Epoch 7, loss: 15989.291020\n",
      "Epoch 8, loss: 15979.231991\n",
      "Epoch 9, loss: 15971.641972\n",
      "Epoch 10, loss: 15956.100823\n",
      "Epoch 11, loss: 15953.687223\n",
      "Epoch 12, loss: 15947.748695\n",
      "Epoch 13, loss: 15945.345770\n",
      "Epoch 14, loss: 15938.055911\n",
      "Epoch 15, loss: 15934.804362\n",
      "Epoch 16, loss: 15929.948381\n",
      "Epoch 17, loss: 15923.618467\n",
      "Epoch 18, loss: 15919.542055\n",
      "Epoch 19, loss: 15916.380397\n",
      "Epoch 20, loss: 15916.010072\n",
      "Epoch 21, loss: 15913.032946\n",
      "Epoch 22, loss: 15908.517763\n",
      "Epoch 23, loss: 15910.871549\n",
      "Epoch 24, loss: 15912.045920\n",
      "Epoch 25, loss: 15903.798705\n",
      "Epoch 26, loss: 15901.653885\n",
      "Epoch 27, loss: 15905.553832\n",
      "Epoch 28, loss: 15901.518066\n",
      "Epoch 29, loss: 15901.907384\n",
      "Epoch 30, loss: 15901.544927\n",
      "Epoch 31, loss: 15896.977559\n",
      "Epoch 32, loss: 15902.889672\n",
      "Epoch 33, loss: 15901.563647\n",
      "Epoch 34, loss: 15902.649999\n",
      "Epoch 35, loss: 15896.255082\n",
      "Epoch 36, loss: 15899.436731\n",
      "Epoch 37, loss: 15895.890691\n",
      "Epoch 38, loss: 15895.719568\n",
      "Epoch 39, loss: 15894.176006\n",
      "Epoch 40, loss: 15896.620884\n",
      "Epoch 41, loss: 15894.482133\n",
      "Epoch 42, loss: 15892.811193\n",
      "Epoch 43, loss: 15896.236066\n",
      "Epoch 44, loss: 15892.221217\n",
      "Epoch 45, loss: 15888.969598\n",
      "Epoch 46, loss: 15886.564016\n",
      "Epoch 47, loss: 15888.248380\n",
      "Epoch 48, loss: 15890.087986\n",
      "Epoch 49, loss: 15889.569857\n",
      "Epoch 50, loss: 15889.220396\n",
      "Epoch 51, loss: 15889.838770\n",
      "Epoch 52, loss: 15889.377029\n",
      "Epoch 53, loss: 15887.705334\n",
      "Epoch 54, loss: 15884.721518\n",
      "Epoch 55, loss: 15884.739234\n",
      "Epoch 56, loss: 15884.727572\n",
      "Epoch 57, loss: 15885.338341\n",
      "Epoch 58, loss: 15887.440860\n",
      "Epoch 59, loss: 15882.748141\n",
      "Epoch 60, loss: 15879.200878\n",
      "Epoch 61, loss: 15877.741156\n",
      "Epoch 62, loss: 15881.504445\n",
      "Epoch 63, loss: 15884.557573\n",
      "Epoch 64, loss: 15877.942826\n",
      "Epoch 65, loss: 15876.453711\n",
      "Epoch 66, loss: 15879.807425\n",
      "Epoch 67, loss: 15875.917659\n",
      "Epoch 68, loss: 15881.656462\n",
      "Epoch 69, loss: 15876.033822\n",
      "Epoch 70, loss: 15882.301595\n",
      "Epoch 71, loss: 15877.003193\n",
      "Epoch 72, loss: 15881.933884\n",
      "Epoch 73, loss: 15877.484824\n",
      "Epoch 74, loss: 15876.270259\n",
      "Epoch 75, loss: 15876.649208\n",
      "Epoch 76, loss: 15874.486894\n",
      "Epoch 77, loss: 15873.148655\n",
      "Epoch 78, loss: 15872.224670\n",
      "Epoch 79, loss: 15873.197332\n",
      "Epoch 80, loss: 15873.695168\n",
      "Epoch 81, loss: 15872.263707\n",
      "Epoch 82, loss: 15872.259906\n",
      "Epoch 83, loss: 15879.975815\n",
      "Epoch 84, loss: 15878.756432\n",
      "Epoch 85, loss: 15872.051119\n",
      "Epoch 86, loss: 15871.652881\n",
      "Epoch 87, loss: 15871.236617\n",
      "Epoch 88, loss: 15873.573319\n",
      "Epoch 89, loss: 15867.839924\n",
      "Epoch 90, loss: 15876.366315\n",
      "Epoch 91, loss: 15878.893172\n",
      "Epoch 92, loss: 15871.155299\n",
      "Epoch 93, loss: 15868.710753\n",
      "Epoch 94, loss: 15873.577068\n",
      "Epoch 95, loss: 15875.188524\n",
      "Epoch 96, loss: 15871.248475\n",
      "Epoch 97, loss: 15868.283756\n",
      "Epoch 98, loss: 15868.794709\n",
      "Epoch 99, loss: 15872.002763\n",
      "Epoch 100, loss: 15869.020089\n",
      "Epoch 101, loss: 15872.588163\n",
      "Epoch 102, loss: 15871.820903\n",
      "Epoch 103, loss: 15872.369265\n",
      "Epoch 104, loss: 15868.524026\n",
      "Epoch 105, loss: 15869.070777\n",
      "Epoch 106, loss: 15866.382655\n",
      "Epoch 107, loss: 15867.233558\n",
      "Epoch 108, loss: 15863.033950\n",
      "Epoch 109, loss: 15869.781825\n",
      "Epoch 110, loss: 15870.484953\n",
      "Epoch 111, loss: 15867.376544\n",
      "Epoch 112, loss: 15868.388225\n",
      "Epoch 113, loss: 15868.445993\n",
      "Epoch 114, loss: 15870.361547\n",
      "Epoch 115, loss: 15865.561598\n",
      "Epoch 116, loss: 15866.409406\n",
      "Epoch 117, loss: 15865.149281\n",
      "Epoch 118, loss: 15867.684798\n",
      "Epoch 119, loss: 15862.300485\n",
      "Epoch 120, loss: 15865.313559\n",
      "Epoch 121, loss: 15862.842904\n",
      "Epoch 122, loss: 15864.384303\n",
      "Epoch 123, loss: 15864.495045\n",
      "Epoch 124, loss: 15865.768595\n",
      "Epoch 125, loss: 15863.206859\n",
      "Epoch 126, loss: 15862.680006\n",
      "Epoch 127, loss: 15860.492649\n",
      "Epoch 128, loss: 15866.907288\n",
      "Epoch 129, loss: 15858.304913\n",
      "Epoch 130, loss: 15860.864997\n",
      "Epoch 131, loss: 15862.413546\n",
      "Epoch 132, loss: 15862.371238\n",
      "Epoch 133, loss: 15861.671964\n",
      "Epoch 134, loss: 15862.908332\n",
      "Epoch 135, loss: 15863.134330\n",
      "Epoch 136, loss: 15864.616839\n",
      "Epoch 137, loss: 15863.951262\n",
      "Epoch 138, loss: 15863.385192\n",
      "Epoch 139, loss: 15857.189234\n",
      "Epoch 140, loss: 15857.979444\n",
      "Epoch 141, loss: 15860.291838\n",
      "Epoch 142, loss: 15858.941867\n",
      "Epoch 143, loss: 15857.048546\n",
      "Epoch 144, loss: 15860.853447\n",
      "Epoch 145, loss: 15858.739988\n",
      "Epoch 146, loss: 15859.265148\n",
      "Epoch 147, loss: 15860.126912\n",
      "Epoch 148, loss: 15859.174542\n",
      "Epoch 149, loss: 15859.727457\n",
      "Epoch 150, loss: 15859.984299\n",
      "Epoch 151, loss: 15859.706374\n",
      "Epoch 152, loss: 15857.051245\n",
      "Epoch 153, loss: 15861.279798\n",
      "Epoch 154, loss: 15857.481636\n",
      "Epoch 155, loss: 15856.006044\n",
      "Epoch 156, loss: 15856.011567\n",
      "Epoch 157, loss: 15857.660312\n",
      "Epoch 158, loss: 15857.834966\n",
      "Epoch 159, loss: 15857.652250\n",
      "Epoch 160, loss: 15857.858310\n",
      "Epoch 161, loss: 15858.700296\n",
      "Epoch 162, loss: 15858.668457\n",
      "Epoch 163, loss: 15857.194685\n",
      "Epoch 164, loss: 15855.528617\n",
      "Epoch 165, loss: 15854.300804\n",
      "Epoch 166, loss: 15858.266530\n",
      "Epoch 167, loss: 15858.124574\n",
      "Epoch 168, loss: 15856.431531\n",
      "Epoch 169, loss: 15853.198732\n",
      "Epoch 170, loss: 15854.589219\n",
      "Epoch 171, loss: 15860.360263\n",
      "Epoch 172, loss: 15853.267879\n",
      "Epoch 173, loss: 15856.615923\n",
      "Epoch 174, loss: 15854.126244\n",
      "Epoch 175, loss: 15856.190133\n",
      "Epoch 176, loss: 15856.895645\n",
      "Epoch 177, loss: 15858.052605\n",
      "Epoch 178, loss: 15854.710042\n",
      "Epoch 179, loss: 15852.516509\n",
      "Epoch 180, loss: 15855.650244\n",
      "Epoch 181, loss: 15851.878425\n",
      "Epoch 182, loss: 15853.247080\n",
      "Epoch 183, loss: 15849.134426\n",
      "Epoch 184, loss: 15854.264746\n",
      "Epoch 185, loss: 15847.750338\n",
      "Epoch 186, loss: 15851.165174\n",
      "Epoch 187, loss: 15844.018946\n",
      "Epoch 188, loss: 15849.404382\n",
      "Epoch 189, loss: 15851.376538\n",
      "Epoch 190, loss: 15853.351034\n",
      "Epoch 191, loss: 15851.940881\n",
      "Epoch 192, loss: 15850.803640\n",
      "Epoch 193, loss: 15849.690662\n",
      "Epoch 194, loss: 15849.345118\n",
      "Epoch 195, loss: 15851.671011\n",
      "Epoch 196, loss: 15851.304014\n",
      "Epoch 197, loss: 15853.840829\n",
      "Epoch 198, loss: 15847.594737\n",
      "Epoch 199, loss: 15850.318294\n",
      "Epoch 0, loss: 15839.152075\n",
      "Epoch 1, loss: 15838.824928\n",
      "Epoch 2, loss: 15835.806225\n",
      "Epoch 3, loss: 15841.298806\n",
      "Epoch 4, loss: 15839.172092\n",
      "Epoch 5, loss: 15836.557706\n",
      "Epoch 6, loss: 15839.132477\n",
      "Epoch 7, loss: 15843.604747\n",
      "Epoch 8, loss: 15836.927460\n",
      "Epoch 9, loss: 15839.925211\n",
      "Epoch 10, loss: 15840.074818\n",
      "Epoch 11, loss: 15842.622798\n",
      "Epoch 12, loss: 15839.580452\n",
      "Epoch 13, loss: 15838.989162\n",
      "Epoch 14, loss: 15839.900668\n",
      "Epoch 15, loss: 15839.470286\n",
      "Epoch 16, loss: 15838.323831\n",
      "Epoch 17, loss: 15839.462536\n",
      "Epoch 18, loss: 15840.627451\n",
      "Epoch 19, loss: 15837.341075\n",
      "Epoch 20, loss: 15843.013246\n",
      "Epoch 21, loss: 15836.040124\n",
      "Epoch 22, loss: 15834.490831\n",
      "Epoch 23, loss: 15837.158023\n",
      "Epoch 24, loss: 15840.076913\n",
      "Epoch 25, loss: 15840.772511\n",
      "Epoch 26, loss: 15834.377252\n",
      "Epoch 27, loss: 15841.558257\n",
      "Epoch 28, loss: 15833.917814\n",
      "Epoch 29, loss: 15834.632394\n",
      "Epoch 30, loss: 15838.565164\n",
      "Epoch 31, loss: 15838.096538\n",
      "Epoch 32, loss: 15836.654194\n",
      "Epoch 33, loss: 15840.114398\n",
      "Epoch 34, loss: 15835.749808\n",
      "Epoch 35, loss: 15831.459448\n",
      "Epoch 36, loss: 15837.002776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, loss: 15835.210204\n",
      "Epoch 38, loss: 15834.908388\n",
      "Epoch 39, loss: 15837.734057\n",
      "Epoch 40, loss: 15838.086335\n",
      "Epoch 41, loss: 15839.119359\n",
      "Epoch 42, loss: 15828.424984\n",
      "Epoch 43, loss: 15838.473245\n",
      "Epoch 44, loss: 15835.293028\n",
      "Epoch 45, loss: 15839.973851\n",
      "Epoch 46, loss: 15832.923420\n",
      "Epoch 47, loss: 15833.928232\n",
      "Epoch 48, loss: 15835.906552\n",
      "Epoch 49, loss: 15834.957783\n",
      "Epoch 50, loss: 15833.609985\n",
      "Epoch 51, loss: 15835.945809\n",
      "Epoch 52, loss: 15835.161593\n",
      "Epoch 53, loss: 15831.101589\n",
      "Epoch 54, loss: 15829.110970\n",
      "Epoch 55, loss: 15831.925047\n",
      "Epoch 56, loss: 15829.323782\n",
      "Epoch 57, loss: 15838.896432\n",
      "Epoch 58, loss: 15838.012791\n",
      "Epoch 59, loss: 15832.303179\n",
      "Epoch 60, loss: 15829.733301\n",
      "Epoch 61, loss: 15834.487586\n",
      "Epoch 62, loss: 15832.017772\n",
      "Epoch 63, loss: 15829.767625\n",
      "Epoch 64, loss: 15834.739782\n",
      "Epoch 65, loss: 15830.035104\n",
      "Epoch 66, loss: 15830.769096\n",
      "Epoch 67, loss: 15829.325711\n",
      "Epoch 68, loss: 15829.722677\n",
      "Epoch 69, loss: 15831.817771\n",
      "Epoch 70, loss: 15828.329081\n",
      "Epoch 71, loss: 15832.791676\n",
      "Epoch 72, loss: 15831.277265\n",
      "Epoch 73, loss: 15829.034575\n",
      "Epoch 74, loss: 15831.553222\n",
      "Epoch 75, loss: 15831.881807\n",
      "Epoch 76, loss: 15832.887692\n",
      "Epoch 77, loss: 15829.782627\n",
      "Epoch 78, loss: 15829.649428\n",
      "Epoch 79, loss: 15831.017202\n",
      "Epoch 80, loss: 15831.301206\n",
      "Epoch 81, loss: 15826.685489\n",
      "Epoch 82, loss: 15830.029413\n",
      "Epoch 83, loss: 15828.842312\n",
      "Epoch 84, loss: 15832.594211\n",
      "Epoch 85, loss: 15827.659299\n",
      "Epoch 86, loss: 15828.902892\n",
      "Epoch 87, loss: 15824.730522\n",
      "Epoch 88, loss: 15828.522477\n",
      "Epoch 89, loss: 15829.082172\n",
      "Epoch 90, loss: 15834.951075\n",
      "Epoch 91, loss: 15826.569261\n",
      "Epoch 92, loss: 15828.022795\n",
      "Epoch 93, loss: 15825.965118\n",
      "Epoch 94, loss: 15826.753860\n",
      "Epoch 95, loss: 15826.647488\n",
      "Epoch 96, loss: 15825.833752\n",
      "Epoch 97, loss: 15829.450360\n",
      "Epoch 98, loss: 15833.488231\n",
      "Epoch 99, loss: 15828.540354\n",
      "Epoch 100, loss: 15824.864549\n",
      "Epoch 101, loss: 15825.667085\n",
      "Epoch 102, loss: 15822.493843\n",
      "Epoch 103, loss: 15831.008960\n",
      "Epoch 104, loss: 15824.226939\n",
      "Epoch 105, loss: 15832.982368\n",
      "Epoch 106, loss: 15822.163411\n",
      "Epoch 107, loss: 15826.629601\n",
      "Epoch 108, loss: 15825.760475\n",
      "Epoch 109, loss: 15825.347870\n",
      "Epoch 110, loss: 15818.359199\n",
      "Epoch 111, loss: 15829.029113\n",
      "Epoch 112, loss: 15824.364692\n",
      "Epoch 113, loss: 15825.939473\n",
      "Epoch 114, loss: 15821.704918\n",
      "Epoch 115, loss: 15831.470045\n",
      "Epoch 116, loss: 15826.938724\n",
      "Epoch 117, loss: 15824.695703\n",
      "Epoch 118, loss: 15823.613060\n",
      "Epoch 119, loss: 15825.942683\n",
      "Epoch 120, loss: 15823.961748\n",
      "Epoch 121, loss: 15822.993451\n",
      "Epoch 122, loss: 15827.754142\n",
      "Epoch 123, loss: 15826.179406\n",
      "Epoch 124, loss: 15822.863449\n",
      "Epoch 125, loss: 15821.823263\n",
      "Epoch 126, loss: 15824.366065\n",
      "Epoch 127, loss: 15825.785291\n",
      "Epoch 128, loss: 15820.642768\n",
      "Epoch 129, loss: 15826.626704\n",
      "Epoch 130, loss: 15819.671824\n",
      "Epoch 131, loss: 15821.856094\n",
      "Epoch 132, loss: 15822.246712\n",
      "Epoch 133, loss: 15824.005130\n",
      "Epoch 134, loss: 15821.162832\n",
      "Epoch 135, loss: 15819.313559\n",
      "Epoch 136, loss: 15824.522087\n",
      "Epoch 137, loss: 15819.706491\n",
      "Epoch 138, loss: 15822.508349\n",
      "Epoch 139, loss: 15826.805035\n",
      "Epoch 140, loss: 15820.078057\n",
      "Epoch 141, loss: 15816.249511\n",
      "Epoch 142, loss: 15824.872984\n",
      "Epoch 143, loss: 15820.901844\n",
      "Epoch 144, loss: 15821.434825\n",
      "Epoch 145, loss: 15822.917326\n",
      "Epoch 146, loss: 15822.130968\n",
      "Epoch 147, loss: 15816.498266\n",
      "Epoch 148, loss: 15815.997013\n",
      "Epoch 149, loss: 15820.835410\n",
      "Epoch 150, loss: 15820.037266\n",
      "Epoch 151, loss: 15818.032319\n",
      "Epoch 152, loss: 15821.274431\n",
      "Epoch 153, loss: 15821.834455\n",
      "Epoch 154, loss: 15814.222093\n",
      "Epoch 155, loss: 15822.206150\n",
      "Epoch 156, loss: 15820.760205\n",
      "Epoch 157, loss: 15818.744501\n",
      "Epoch 158, loss: 15819.109700\n",
      "Epoch 159, loss: 15819.621873\n",
      "Epoch 160, loss: 15818.398011\n",
      "Epoch 161, loss: 15822.494593\n",
      "Epoch 162, loss: 15820.358054\n",
      "Epoch 163, loss: 15817.602028\n",
      "Epoch 164, loss: 15819.103013\n",
      "Epoch 165, loss: 15815.221157\n",
      "Epoch 166, loss: 15819.322631\n",
      "Epoch 167, loss: 15816.646688\n",
      "Epoch 168, loss: 15819.238445\n",
      "Epoch 169, loss: 15815.158294\n",
      "Epoch 170, loss: 15819.564793\n",
      "Epoch 171, loss: 15818.678001\n",
      "Epoch 172, loss: 15811.784135\n",
      "Epoch 173, loss: 15815.786268\n",
      "Epoch 174, loss: 15818.588559\n",
      "Epoch 175, loss: 15816.040597\n",
      "Epoch 176, loss: 15816.644511\n",
      "Epoch 177, loss: 15817.089020\n",
      "Epoch 178, loss: 15815.519494\n",
      "Epoch 179, loss: 15817.763104\n",
      "Epoch 180, loss: 15812.535187\n",
      "Epoch 181, loss: 15817.725013\n",
      "Epoch 182, loss: 15815.088920\n",
      "Epoch 183, loss: 15816.015091\n",
      "Epoch 184, loss: 15814.901436\n",
      "Epoch 185, loss: 15814.829389\n",
      "Epoch 186, loss: 15817.751643\n",
      "Epoch 187, loss: 15819.702454\n",
      "Epoch 188, loss: 15812.082155\n",
      "Epoch 189, loss: 15816.126533\n",
      "Epoch 190, loss: 15816.079941\n",
      "Epoch 191, loss: 15816.664353\n",
      "Epoch 192, loss: 15817.791833\n",
      "Epoch 193, loss: 15815.998842\n",
      "Epoch 194, loss: 15810.916005\n",
      "Epoch 195, loss: 15811.175190\n",
      "Epoch 196, loss: 15813.526831\n",
      "Epoch 197, loss: 15810.947988\n",
      "Epoch 198, loss: 15819.308853\n",
      "Epoch 199, loss: 15812.388833\n",
      "Epoch 0, loss: 15811.702175\n",
      "Epoch 1, loss: 15813.368074\n",
      "Epoch 2, loss: 15813.494756\n",
      "Epoch 3, loss: 15811.570125\n",
      "Epoch 4, loss: 15813.432077\n",
      "Epoch 5, loss: 15809.185854\n",
      "Epoch 6, loss: 15817.580774\n",
      "Epoch 7, loss: 15810.189806\n",
      "Epoch 8, loss: 15812.595632\n",
      "Epoch 9, loss: 15814.359033\n",
      "Epoch 10, loss: 15814.032211\n",
      "Epoch 11, loss: 15814.375525\n",
      "Epoch 12, loss: 15811.798916\n",
      "Epoch 13, loss: 15811.910731\n",
      "Epoch 14, loss: 15810.006391\n",
      "Epoch 15, loss: 15818.472846\n",
      "Epoch 16, loss: 15808.914105\n",
      "Epoch 17, loss: 15812.844206\n",
      "Epoch 18, loss: 15811.169357\n",
      "Epoch 19, loss: 15814.197111\n",
      "Epoch 20, loss: 15812.507367\n",
      "Epoch 21, loss: 15808.909201\n",
      "Epoch 22, loss: 15813.741796\n",
      "Epoch 23, loss: 15811.666134\n",
      "Epoch 24, loss: 15813.279092\n",
      "Epoch 25, loss: 15808.375252\n",
      "Epoch 26, loss: 15809.330486\n",
      "Epoch 27, loss: 15811.417576\n",
      "Epoch 28, loss: 15807.653516\n",
      "Epoch 29, loss: 15809.784025\n",
      "Epoch 30, loss: 15808.155302\n",
      "Epoch 31, loss: 15807.194038\n",
      "Epoch 32, loss: 15811.318922\n",
      "Epoch 33, loss: 15806.013877\n",
      "Epoch 34, loss: 15808.041518\n",
      "Epoch 35, loss: 15810.002916\n",
      "Epoch 36, loss: 15809.785962\n",
      "Epoch 37, loss: 15808.404412\n",
      "Epoch 38, loss: 15806.001828\n",
      "Epoch 39, loss: 15809.435620\n",
      "Epoch 40, loss: 15810.233152\n",
      "Epoch 41, loss: 15811.817366\n",
      "Epoch 42, loss: 15805.259449\n",
      "Epoch 43, loss: 15813.046412\n",
      "Epoch 44, loss: 15810.095140\n",
      "Epoch 45, loss: 15808.423406\n",
      "Epoch 46, loss: 15803.614280\n",
      "Epoch 47, loss: 15809.750253\n",
      "Epoch 48, loss: 15807.163647\n",
      "Epoch 49, loss: 15807.359181\n",
      "Epoch 50, loss: 15806.074501\n",
      "Epoch 51, loss: 15806.698070\n",
      "Epoch 52, loss: 15807.351278\n",
      "Epoch 53, loss: 15811.523462\n",
      "Epoch 54, loss: 15807.241918\n",
      "Epoch 55, loss: 15806.085822\n",
      "Epoch 56, loss: 15811.795320\n",
      "Epoch 57, loss: 15806.006423\n",
      "Epoch 58, loss: 15807.454729\n",
      "Epoch 59, loss: 15804.856235\n",
      "Epoch 60, loss: 15808.267393\n",
      "Epoch 61, loss: 15805.888671\n",
      "Epoch 62, loss: 15805.011020\n",
      "Epoch 63, loss: 15804.094845\n",
      "Epoch 64, loss: 15809.535477\n",
      "Epoch 65, loss: 15805.801185\n",
      "Epoch 66, loss: 15809.191966\n",
      "Epoch 67, loss: 15808.101018\n",
      "Epoch 68, loss: 15805.557136\n",
      "Epoch 69, loss: 15805.519703\n",
      "Epoch 70, loss: 15802.744047\n",
      "Epoch 71, loss: 15807.015636\n",
      "Epoch 72, loss: 15805.562171\n",
      "Epoch 73, loss: 15805.887915\n",
      "Epoch 74, loss: 15801.146941\n",
      "Epoch 75, loss: 15802.582641\n",
      "Epoch 76, loss: 15805.765470\n",
      "Epoch 77, loss: 15801.608289\n",
      "Epoch 78, loss: 15806.922176\n",
      "Epoch 79, loss: 15801.974009\n",
      "Epoch 80, loss: 15806.197566\n",
      "Epoch 81, loss: 15804.671095\n",
      "Epoch 82, loss: 15805.140842\n",
      "Epoch 83, loss: 15811.088726\n",
      "Epoch 84, loss: 15802.463352\n",
      "Epoch 85, loss: 15807.165760\n",
      "Epoch 86, loss: 15804.010670\n",
      "Epoch 87, loss: 15802.714140\n",
      "Epoch 88, loss: 15812.039988\n",
      "Epoch 89, loss: 15808.838718\n",
      "Epoch 90, loss: 15802.365598\n",
      "Epoch 91, loss: 15800.696229\n",
      "Epoch 92, loss: 15810.341898\n",
      "Epoch 93, loss: 15802.753642\n",
      "Epoch 94, loss: 15803.548423\n",
      "Epoch 95, loss: 15802.305152\n",
      "Epoch 96, loss: 15801.339675\n",
      "Epoch 97, loss: 15800.992259\n",
      "Epoch 98, loss: 15799.812199\n",
      "Epoch 99, loss: 15797.792338\n",
      "Epoch 100, loss: 15803.996487\n",
      "Epoch 101, loss: 15800.536099\n",
      "Epoch 102, loss: 15804.338379\n",
      "Epoch 103, loss: 15799.541635\n",
      "Epoch 104, loss: 15804.793059\n",
      "Epoch 105, loss: 15802.738714\n",
      "Epoch 106, loss: 15800.500540\n",
      "Epoch 107, loss: 15804.577838\n",
      "Epoch 108, loss: 15800.035925\n",
      "Epoch 109, loss: 15805.551210\n",
      "Epoch 110, loss: 15802.385958\n",
      "Epoch 111, loss: 15797.929881\n",
      "Epoch 112, loss: 15797.231224\n",
      "Epoch 113, loss: 15804.807767\n",
      "Epoch 114, loss: 15798.818666\n",
      "Epoch 115, loss: 15801.618838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116, loss: 15796.737077\n",
      "Epoch 117, loss: 15797.067012\n",
      "Epoch 118, loss: 15800.446384\n",
      "Epoch 119, loss: 15800.214334\n",
      "Epoch 120, loss: 15800.117062\n",
      "Epoch 121, loss: 15799.912835\n",
      "Epoch 122, loss: 15799.507045\n",
      "Epoch 123, loss: 15793.520028\n",
      "Epoch 124, loss: 15798.422691\n",
      "Epoch 125, loss: 15801.010821\n",
      "Epoch 126, loss: 15799.435213\n",
      "Epoch 127, loss: 15795.325218\n",
      "Epoch 128, loss: 15798.519865\n",
      "Epoch 129, loss: 15798.204165\n",
      "Epoch 130, loss: 15797.028999\n",
      "Epoch 131, loss: 15801.453572\n",
      "Epoch 132, loss: 15795.740256\n",
      "Epoch 133, loss: 15800.168582\n",
      "Epoch 134, loss: 15797.047738\n",
      "Epoch 135, loss: 15798.740227\n",
      "Epoch 136, loss: 15799.077353\n",
      "Epoch 137, loss: 15796.731874\n",
      "Epoch 138, loss: 15797.341880\n",
      "Epoch 139, loss: 15799.902697\n",
      "Epoch 140, loss: 15798.804224\n",
      "Epoch 141, loss: 15801.461266\n",
      "Epoch 142, loss: 15801.278833\n",
      "Epoch 143, loss: 15793.627315\n",
      "Epoch 144, loss: 15799.526113\n",
      "Epoch 145, loss: 15796.370502\n",
      "Epoch 146, loss: 15796.902238\n",
      "Epoch 147, loss: 15803.963735\n",
      "Epoch 148, loss: 15796.068526\n",
      "Epoch 149, loss: 15794.787388\n",
      "Epoch 150, loss: 15797.166913\n",
      "Epoch 151, loss: 15796.432658\n",
      "Epoch 152, loss: 15797.789640\n",
      "Epoch 153, loss: 15791.918733\n",
      "Epoch 154, loss: 15793.922466\n",
      "Epoch 155, loss: 15796.769786\n",
      "Epoch 156, loss: 15799.659952\n",
      "Epoch 157, loss: 15798.430740\n",
      "Epoch 158, loss: 15791.245165\n",
      "Epoch 159, loss: 15797.962468\n",
      "Epoch 160, loss: 15796.973537\n",
      "Epoch 161, loss: 15793.412212\n",
      "Epoch 162, loss: 15795.596140\n",
      "Epoch 163, loss: 15798.655295\n",
      "Epoch 164, loss: 15794.913267\n",
      "Epoch 165, loss: 15792.783366\n",
      "Epoch 166, loss: 15790.969138\n",
      "Epoch 167, loss: 15793.495850\n",
      "Epoch 168, loss: 15794.652115\n",
      "Epoch 169, loss: 15793.446769\n",
      "Epoch 170, loss: 15796.964709\n",
      "Epoch 171, loss: 15794.163510\n",
      "Epoch 172, loss: 15794.918187\n",
      "Epoch 173, loss: 15796.522948\n",
      "Epoch 174, loss: 15795.004677\n",
      "Epoch 175, loss: 15792.206641\n",
      "Epoch 176, loss: 15794.271640\n",
      "Epoch 177, loss: 15790.809610\n",
      "Epoch 178, loss: 15795.457204\n",
      "Epoch 179, loss: 15793.645716\n",
      "Epoch 180, loss: 15799.520877\n",
      "Epoch 181, loss: 15796.558334\n",
      "Epoch 182, loss: 15793.276726\n",
      "Epoch 183, loss: 15796.561727\n",
      "Epoch 184, loss: 15794.284122\n",
      "Epoch 185, loss: 15793.170618\n",
      "Epoch 186, loss: 15802.068486\n",
      "Epoch 187, loss: 15793.357263\n",
      "Epoch 188, loss: 15794.600975\n",
      "Epoch 189, loss: 15792.024500\n",
      "Epoch 190, loss: 15791.173339\n",
      "Epoch 191, loss: 15791.770639\n",
      "Epoch 192, loss: 15789.700429\n",
      "Epoch 193, loss: 15792.112922\n",
      "Epoch 194, loss: 15794.175774\n",
      "Epoch 195, loss: 15793.927240\n",
      "Epoch 196, loss: 15792.565860\n",
      "Epoch 197, loss: 15794.324088\n",
      "Epoch 198, loss: 15794.056859\n",
      "Epoch 199, loss: 15796.481531\n",
      "Epoch 0, loss: 15777.154198\n",
      "Epoch 1, loss: 15774.939080\n",
      "Epoch 2, loss: 15774.795472\n",
      "Epoch 3, loss: 15774.402250\n",
      "Epoch 4, loss: 15774.509436\n",
      "Epoch 5, loss: 15774.611942\n",
      "Epoch 6, loss: 15774.444219\n",
      "Epoch 7, loss: 15774.571421\n",
      "Epoch 8, loss: 15774.905122\n",
      "Epoch 9, loss: 15774.742283\n",
      "Epoch 10, loss: 15774.627622\n",
      "Epoch 11, loss: 15774.295684\n",
      "Epoch 12, loss: 15774.561366\n",
      "Epoch 13, loss: 15774.111158\n",
      "Epoch 14, loss: 15774.624264\n",
      "Epoch 15, loss: 15774.343208\n",
      "Epoch 16, loss: 15774.772178\n",
      "Epoch 17, loss: 15774.123203\n",
      "Epoch 18, loss: 15774.220961\n",
      "Epoch 19, loss: 15774.479570\n",
      "Epoch 20, loss: 15774.255265\n",
      "Epoch 21, loss: 15774.628197\n",
      "Epoch 22, loss: 15774.244164\n",
      "Epoch 23, loss: 15773.963968\n",
      "Epoch 24, loss: 15774.019228\n",
      "Epoch 25, loss: 15774.348652\n",
      "Epoch 26, loss: 15774.253614\n",
      "Epoch 27, loss: 15774.159733\n",
      "Epoch 28, loss: 15774.063194\n",
      "Epoch 29, loss: 15774.385591\n",
      "Epoch 30, loss: 15774.371769\n",
      "Epoch 31, loss: 15774.576634\n",
      "Epoch 32, loss: 15774.201709\n",
      "Epoch 33, loss: 15774.070186\n",
      "Epoch 34, loss: 15774.560882\n",
      "Epoch 35, loss: 15774.534329\n",
      "Epoch 36, loss: 15774.132348\n",
      "Epoch 37, loss: 15773.965224\n",
      "Epoch 38, loss: 15774.230750\n",
      "Epoch 39, loss: 15774.261794\n",
      "Epoch 40, loss: 15774.090321\n",
      "Epoch 41, loss: 15774.252951\n",
      "Epoch 42, loss: 15774.196664\n",
      "Epoch 43, loss: 15774.027271\n",
      "Epoch 44, loss: 15773.844576\n",
      "Epoch 45, loss: 15774.441479\n",
      "Epoch 46, loss: 15773.903254\n",
      "Epoch 47, loss: 15774.120323\n",
      "Epoch 48, loss: 15774.129899\n",
      "Epoch 49, loss: 15773.989365\n",
      "Epoch 50, loss: 15774.365318\n",
      "Epoch 51, loss: 15774.305787\n",
      "Epoch 52, loss: 15773.573580\n",
      "Epoch 53, loss: 15773.908610\n",
      "Epoch 54, loss: 15773.828831\n",
      "Epoch 55, loss: 15774.281765\n",
      "Epoch 56, loss: 15773.955344\n",
      "Epoch 57, loss: 15773.970159\n",
      "Epoch 58, loss: 15773.549926\n",
      "Epoch 59, loss: 15773.996319\n",
      "Epoch 60, loss: 15773.863105\n",
      "Epoch 61, loss: 15773.705955\n",
      "Epoch 62, loss: 15774.048704\n",
      "Epoch 63, loss: 15773.784932\n",
      "Epoch 64, loss: 15774.126023\n",
      "Epoch 65, loss: 15773.741912\n",
      "Epoch 66, loss: 15774.080881\n",
      "Epoch 67, loss: 15773.799223\n",
      "Epoch 68, loss: 15773.943798\n",
      "Epoch 69, loss: 15773.757532\n",
      "Epoch 70, loss: 15773.948414\n",
      "Epoch 71, loss: 15773.788699\n",
      "Epoch 72, loss: 15773.494826\n",
      "Epoch 73, loss: 15773.768950\n",
      "Epoch 74, loss: 15773.879913\n",
      "Epoch 75, loss: 15773.648032\n",
      "Epoch 76, loss: 15774.208042\n",
      "Epoch 77, loss: 15773.908779\n",
      "Epoch 78, loss: 15773.493501\n",
      "Epoch 79, loss: 15773.767640\n",
      "Epoch 80, loss: 15773.545948\n",
      "Epoch 81, loss: 15773.724839\n",
      "Epoch 82, loss: 15773.356689\n",
      "Epoch 83, loss: 15774.236650\n",
      "Epoch 84, loss: 15774.259531\n",
      "Epoch 85, loss: 15773.323869\n",
      "Epoch 86, loss: 15773.984117\n",
      "Epoch 87, loss: 15773.744167\n",
      "Epoch 88, loss: 15773.617108\n",
      "Epoch 89, loss: 15773.283835\n",
      "Epoch 90, loss: 15773.474217\n",
      "Epoch 91, loss: 15773.422918\n",
      "Epoch 92, loss: 15773.781643\n",
      "Epoch 93, loss: 15773.642624\n",
      "Epoch 94, loss: 15773.967035\n",
      "Epoch 95, loss: 15773.533944\n",
      "Epoch 96, loss: 15773.779707\n",
      "Epoch 97, loss: 15773.770190\n",
      "Epoch 98, loss: 15773.778477\n",
      "Epoch 99, loss: 15773.294722\n",
      "Epoch 100, loss: 15773.847573\n",
      "Epoch 101, loss: 15773.507773\n",
      "Epoch 102, loss: 15773.812400\n",
      "Epoch 103, loss: 15773.738626\n",
      "Epoch 104, loss: 15773.414741\n",
      "Epoch 105, loss: 15773.794511\n",
      "Epoch 106, loss: 15773.455854\n",
      "Epoch 107, loss: 15773.514642\n",
      "Epoch 108, loss: 15773.543553\n",
      "Epoch 109, loss: 15773.103466\n",
      "Epoch 110, loss: 15773.285190\n",
      "Epoch 111, loss: 15773.316967\n",
      "Epoch 112, loss: 15773.569162\n",
      "Epoch 113, loss: 15773.710673\n",
      "Epoch 114, loss: 15773.633645\n",
      "Epoch 115, loss: 15773.921362\n",
      "Epoch 116, loss: 15773.467687\n",
      "Epoch 117, loss: 15773.318933\n",
      "Epoch 118, loss: 15773.110743\n",
      "Epoch 119, loss: 15773.590747\n",
      "Epoch 120, loss: 15773.468517\n",
      "Epoch 121, loss: 15773.601508\n",
      "Epoch 122, loss: 15773.326842\n",
      "Epoch 123, loss: 15773.231809\n",
      "Epoch 124, loss: 15773.137883\n",
      "Epoch 125, loss: 15773.198101\n",
      "Epoch 126, loss: 15773.108496\n",
      "Epoch 127, loss: 15773.180417\n",
      "Epoch 128, loss: 15772.935293\n",
      "Epoch 129, loss: 15773.143102\n",
      "Epoch 130, loss: 15773.384660\n",
      "Epoch 131, loss: 15772.908398\n",
      "Epoch 132, loss: 15773.293655\n",
      "Epoch 133, loss: 15773.245619\n",
      "Epoch 134, loss: 15773.407968\n",
      "Epoch 135, loss: 15773.356894\n",
      "Epoch 136, loss: 15773.067756\n",
      "Epoch 137, loss: 15772.803237\n",
      "Epoch 138, loss: 15773.114090\n",
      "Epoch 139, loss: 15772.879811\n",
      "Epoch 140, loss: 15773.202481\n",
      "Epoch 141, loss: 15773.343871\n",
      "Epoch 142, loss: 15773.363454\n",
      "Epoch 143, loss: 15773.288562\n",
      "Epoch 144, loss: 15773.305947\n",
      "Epoch 145, loss: 15773.141653\n",
      "Epoch 146, loss: 15773.172906\n",
      "Epoch 147, loss: 15773.201253\n",
      "Epoch 148, loss: 15773.434065\n",
      "Epoch 149, loss: 15773.104480\n",
      "Epoch 150, loss: 15773.270477\n",
      "Epoch 151, loss: 15772.623551\n",
      "Epoch 152, loss: 15772.933062\n",
      "Epoch 153, loss: 15773.199683\n",
      "Epoch 154, loss: 15773.119219\n",
      "Epoch 155, loss: 15772.865631\n",
      "Epoch 156, loss: 15773.310565\n",
      "Epoch 157, loss: 15772.695049\n",
      "Epoch 158, loss: 15772.722192\n",
      "Epoch 159, loss: 15772.875195\n",
      "Epoch 160, loss: 15773.075081\n",
      "Epoch 161, loss: 15772.844639\n",
      "Epoch 162, loss: 15772.990271\n",
      "Epoch 163, loss: 15772.827023\n",
      "Epoch 164, loss: 15773.031944\n",
      "Epoch 165, loss: 15772.881592\n",
      "Epoch 166, loss: 15773.125099\n",
      "Epoch 167, loss: 15773.141065\n",
      "Epoch 168, loss: 15773.089659\n",
      "Epoch 169, loss: 15772.550540\n",
      "Epoch 170, loss: 15772.756201\n",
      "Epoch 171, loss: 15773.034841\n",
      "Epoch 172, loss: 15772.761878\n",
      "Epoch 173, loss: 15772.679827\n",
      "Epoch 174, loss: 15773.369920\n",
      "Epoch 175, loss: 15772.889929\n",
      "Epoch 176, loss: 15772.802457\n",
      "Epoch 177, loss: 15772.914914\n",
      "Epoch 178, loss: 15773.221951\n",
      "Epoch 179, loss: 15772.845693\n",
      "Epoch 180, loss: 15772.296041\n",
      "Epoch 181, loss: 15773.001608\n",
      "Epoch 182, loss: 15772.770871\n",
      "Epoch 183, loss: 15772.721757\n",
      "Epoch 184, loss: 15773.161542\n",
      "Epoch 185, loss: 15772.647690\n",
      "Epoch 186, loss: 15772.525488\n",
      "Epoch 187, loss: 15772.447780\n",
      "Epoch 188, loss: 15772.671357\n",
      "Epoch 189, loss: 15772.665968\n",
      "Epoch 190, loss: 15772.445231\n",
      "Epoch 191, loss: 15772.971867\n",
      "Epoch 192, loss: 15772.879101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193, loss: 15773.073274\n",
      "Epoch 194, loss: 15772.405301\n",
      "Epoch 195, loss: 15772.829675\n",
      "Epoch 196, loss: 15772.330094\n",
      "Epoch 197, loss: 15772.701896\n",
      "Epoch 198, loss: 15772.530415\n",
      "Epoch 199, loss: 15772.634547\n",
      "Epoch 0, loss: 15763.867294\n",
      "Epoch 1, loss: 15763.861743\n",
      "Epoch 2, loss: 15764.258726\n",
      "Epoch 3, loss: 15764.087006\n",
      "Epoch 4, loss: 15764.185243\n",
      "Epoch 5, loss: 15764.023025\n",
      "Epoch 6, loss: 15764.017931\n",
      "Epoch 7, loss: 15764.197441\n",
      "Epoch 8, loss: 15764.135849\n",
      "Epoch 9, loss: 15764.308350\n",
      "Epoch 10, loss: 15764.459564\n",
      "Epoch 11, loss: 15763.774918\n",
      "Epoch 12, loss: 15764.439473\n",
      "Epoch 13, loss: 15763.655775\n",
      "Epoch 14, loss: 15763.978193\n",
      "Epoch 15, loss: 15763.660996\n",
      "Epoch 16, loss: 15763.782117\n",
      "Epoch 17, loss: 15764.238345\n",
      "Epoch 18, loss: 15763.777702\n",
      "Epoch 19, loss: 15763.885920\n",
      "Epoch 20, loss: 15763.901720\n",
      "Epoch 21, loss: 15764.040958\n",
      "Epoch 22, loss: 15763.715308\n",
      "Epoch 23, loss: 15763.842705\n",
      "Epoch 24, loss: 15763.953604\n",
      "Epoch 25, loss: 15763.825232\n",
      "Epoch 26, loss: 15763.546664\n",
      "Epoch 27, loss: 15763.767653\n",
      "Epoch 28, loss: 15764.207636\n",
      "Epoch 29, loss: 15763.862779\n",
      "Epoch 30, loss: 15763.730024\n",
      "Epoch 31, loss: 15763.839881\n",
      "Epoch 32, loss: 15763.676777\n",
      "Epoch 33, loss: 15763.408840\n",
      "Epoch 34, loss: 15763.930648\n",
      "Epoch 35, loss: 15763.686965\n",
      "Epoch 36, loss: 15763.826919\n",
      "Epoch 37, loss: 15763.701557\n",
      "Epoch 38, loss: 15763.763433\n",
      "Epoch 39, loss: 15763.875176\n",
      "Epoch 40, loss: 15764.247677\n",
      "Epoch 41, loss: 15763.755754\n",
      "Epoch 42, loss: 15763.512841\n",
      "Epoch 43, loss: 15763.492870\n",
      "Epoch 44, loss: 15763.640516\n",
      "Epoch 45, loss: 15763.655086\n",
      "Epoch 46, loss: 15763.275971\n",
      "Epoch 47, loss: 15763.657285\n",
      "Epoch 48, loss: 15763.400119\n",
      "Epoch 49, loss: 15763.729419\n",
      "Epoch 50, loss: 15763.824083\n",
      "Epoch 51, loss: 15763.288241\n",
      "Epoch 52, loss: 15763.444456\n",
      "Epoch 53, loss: 15763.762607\n",
      "Epoch 54, loss: 15763.556813\n",
      "Epoch 55, loss: 15763.579144\n",
      "Epoch 56, loss: 15763.318286\n",
      "Epoch 57, loss: 15763.897216\n",
      "Epoch 58, loss: 15763.441409\n",
      "Epoch 59, loss: 15763.142038\n",
      "Epoch 60, loss: 15763.542204\n",
      "Epoch 61, loss: 15763.371582\n",
      "Epoch 62, loss: 15763.945393\n",
      "Epoch 63, loss: 15763.268426\n",
      "Epoch 64, loss: 15763.409510\n",
      "Epoch 65, loss: 15763.450400\n",
      "Epoch 66, loss: 15763.535878\n",
      "Epoch 67, loss: 15763.684519\n",
      "Epoch 68, loss: 15763.410372\n",
      "Epoch 69, loss: 15763.606058\n",
      "Epoch 70, loss: 15763.534404\n",
      "Epoch 71, loss: 15763.667608\n",
      "Epoch 72, loss: 15763.515953\n",
      "Epoch 73, loss: 15763.219358\n",
      "Epoch 74, loss: 15763.640828\n",
      "Epoch 75, loss: 15763.259449\n",
      "Epoch 76, loss: 15762.778923\n",
      "Epoch 77, loss: 15763.345985\n",
      "Epoch 78, loss: 15763.315931\n",
      "Epoch 79, loss: 15763.349041\n",
      "Epoch 80, loss: 15763.711616\n",
      "Epoch 81, loss: 15763.847095\n",
      "Epoch 82, loss: 15763.581040\n",
      "Epoch 83, loss: 15763.195118\n",
      "Epoch 84, loss: 15763.264268\n",
      "Epoch 85, loss: 15763.145187\n",
      "Epoch 86, loss: 15763.138068\n",
      "Epoch 87, loss: 15763.029046\n",
      "Epoch 88, loss: 15763.524097\n",
      "Epoch 89, loss: 15763.753975\n",
      "Epoch 90, loss: 15763.277991\n",
      "Epoch 91, loss: 15763.265638\n",
      "Epoch 92, loss: 15763.351273\n",
      "Epoch 93, loss: 15763.423145\n",
      "Epoch 94, loss: 15763.311426\n",
      "Epoch 95, loss: 15763.283577\n",
      "Epoch 96, loss: 15763.287153\n",
      "Epoch 97, loss: 15763.681771\n",
      "Epoch 98, loss: 15763.258152\n",
      "Epoch 99, loss: 15763.242975\n",
      "Epoch 100, loss: 15762.919381\n",
      "Epoch 101, loss: 15763.195533\n",
      "Epoch 102, loss: 15763.298581\n",
      "Epoch 103, loss: 15763.039109\n",
      "Epoch 104, loss: 15763.023343\n",
      "Epoch 105, loss: 15762.979759\n",
      "Epoch 106, loss: 15763.326077\n",
      "Epoch 107, loss: 15763.101161\n",
      "Epoch 108, loss: 15763.094669\n",
      "Epoch 109, loss: 15762.914312\n",
      "Epoch 110, loss: 15762.633016\n",
      "Epoch 111, loss: 15763.326363\n",
      "Epoch 112, loss: 15763.002773\n",
      "Epoch 113, loss: 15762.961053\n",
      "Epoch 114, loss: 15762.841902\n",
      "Epoch 115, loss: 15762.867325\n",
      "Epoch 116, loss: 15763.080972\n",
      "Epoch 117, loss: 15763.297787\n",
      "Epoch 118, loss: 15763.047852\n",
      "Epoch 119, loss: 15763.263825\n",
      "Epoch 120, loss: 15762.793937\n",
      "Epoch 121, loss: 15763.021053\n",
      "Epoch 122, loss: 15762.908797\n",
      "Epoch 123, loss: 15762.636686\n",
      "Epoch 124, loss: 15762.928813\n",
      "Epoch 125, loss: 15762.522205\n",
      "Epoch 126, loss: 15763.163856\n",
      "Epoch 127, loss: 15763.164276\n",
      "Epoch 128, loss: 15762.557211\n",
      "Epoch 129, loss: 15762.889792\n",
      "Epoch 130, loss: 15763.080199\n",
      "Epoch 131, loss: 15762.758540\n",
      "Epoch 132, loss: 15762.900758\n",
      "Epoch 133, loss: 15762.896749\n",
      "Epoch 134, loss: 15763.002733\n",
      "Epoch 135, loss: 15762.645598\n",
      "Epoch 136, loss: 15762.735336\n",
      "Epoch 137, loss: 15762.726292\n",
      "Epoch 138, loss: 15762.877033\n",
      "Epoch 139, loss: 15762.533301\n",
      "Epoch 140, loss: 15762.770753\n",
      "Epoch 141, loss: 15763.191688\n",
      "Epoch 142, loss: 15762.880535\n",
      "Epoch 143, loss: 15762.695986\n",
      "Epoch 144, loss: 15762.339312\n",
      "Epoch 145, loss: 15762.532407\n",
      "Epoch 146, loss: 15762.664818\n",
      "Epoch 147, loss: 15762.565441\n",
      "Epoch 148, loss: 15762.541025\n",
      "Epoch 149, loss: 15762.302866\n",
      "Epoch 150, loss: 15762.685980\n",
      "Epoch 151, loss: 15762.387899\n",
      "Epoch 152, loss: 15762.389179\n",
      "Epoch 153, loss: 15762.613056\n",
      "Epoch 154, loss: 15762.800474\n",
      "Epoch 155, loss: 15762.720855\n",
      "Epoch 156, loss: 15762.699257\n",
      "Epoch 157, loss: 15762.985208\n",
      "Epoch 158, loss: 15762.455902\n",
      "Epoch 159, loss: 15762.616623\n",
      "Epoch 160, loss: 15762.584044\n",
      "Epoch 161, loss: 15762.972604\n",
      "Epoch 162, loss: 15762.619977\n",
      "Epoch 163, loss: 15762.362031\n",
      "Epoch 164, loss: 15762.691938\n",
      "Epoch 165, loss: 15762.389221\n",
      "Epoch 166, loss: 15762.552671\n",
      "Epoch 167, loss: 15762.501170\n",
      "Epoch 168, loss: 15762.448686\n",
      "Epoch 169, loss: 15762.306611\n",
      "Epoch 170, loss: 15762.449080\n",
      "Epoch 171, loss: 15762.446703\n",
      "Epoch 172, loss: 15762.309483\n",
      "Epoch 173, loss: 15762.229628\n",
      "Epoch 174, loss: 15762.604020\n",
      "Epoch 175, loss: 15762.433818\n",
      "Epoch 176, loss: 15762.469944\n",
      "Epoch 177, loss: 15762.479264\n",
      "Epoch 178, loss: 15762.330683\n",
      "Epoch 179, loss: 15762.258795\n",
      "Epoch 180, loss: 15762.465519\n",
      "Epoch 181, loss: 15762.001261\n",
      "Epoch 182, loss: 15762.673735\n",
      "Epoch 183, loss: 15762.276911\n",
      "Epoch 184, loss: 15762.505680\n",
      "Epoch 185, loss: 15762.210857\n",
      "Epoch 186, loss: 15761.773768\n",
      "Epoch 187, loss: 15761.833797\n",
      "Epoch 188, loss: 15762.416626\n",
      "Epoch 189, loss: 15762.466295\n",
      "Epoch 190, loss: 15762.400302\n",
      "Epoch 191, loss: 15762.230041\n",
      "Epoch 192, loss: 15762.065260\n",
      "Epoch 193, loss: 15762.234836\n",
      "Epoch 194, loss: 15762.563906\n",
      "Epoch 195, loss: 15762.494231\n",
      "Epoch 196, loss: 15762.349508\n",
      "Epoch 197, loss: 15761.584549\n",
      "Epoch 198, loss: 15762.614675\n",
      "Epoch 199, loss: 15762.113656\n",
      "Epoch 0, loss: 15761.193229\n",
      "Epoch 1, loss: 15761.439174\n",
      "Epoch 2, loss: 15761.074107\n",
      "Epoch 3, loss: 15761.440231\n",
      "Epoch 4, loss: 15761.074828\n",
      "Epoch 5, loss: 15761.374209\n",
      "Epoch 6, loss: 15761.380731\n",
      "Epoch 7, loss: 15760.969415\n",
      "Epoch 8, loss: 15760.950572\n",
      "Epoch 9, loss: 15761.428106\n",
      "Epoch 10, loss: 15761.362747\n",
      "Epoch 11, loss: 15761.356569\n",
      "Epoch 12, loss: 15761.097202\n",
      "Epoch 13, loss: 15761.276611\n",
      "Epoch 14, loss: 15760.888138\n",
      "Epoch 15, loss: 15761.039757\n",
      "Epoch 16, loss: 15761.321323\n",
      "Epoch 17, loss: 15761.258528\n",
      "Epoch 18, loss: 15761.247061\n",
      "Epoch 19, loss: 15761.327105\n",
      "Epoch 20, loss: 15760.845329\n",
      "Epoch 21, loss: 15761.254672\n",
      "Epoch 22, loss: 15761.308521\n",
      "Epoch 23, loss: 15761.142187\n",
      "Epoch 24, loss: 15760.938735\n",
      "Epoch 25, loss: 15760.887320\n",
      "Epoch 26, loss: 15761.089426\n",
      "Epoch 27, loss: 15761.385500\n",
      "Epoch 28, loss: 15760.806056\n",
      "Epoch 29, loss: 15761.175454\n",
      "Epoch 30, loss: 15761.021125\n",
      "Epoch 31, loss: 15761.228318\n",
      "Epoch 32, loss: 15760.900880\n",
      "Epoch 33, loss: 15761.190036\n",
      "Epoch 34, loss: 15760.834925\n",
      "Epoch 35, loss: 15761.104985\n",
      "Epoch 36, loss: 15760.968714\n",
      "Epoch 37, loss: 15761.049482\n",
      "Epoch 38, loss: 15761.238153\n",
      "Epoch 39, loss: 15761.171026\n",
      "Epoch 40, loss: 15760.830082\n",
      "Epoch 41, loss: 15760.602013\n",
      "Epoch 42, loss: 15760.931895\n",
      "Epoch 43, loss: 15760.764322\n",
      "Epoch 44, loss: 15761.049628\n",
      "Epoch 45, loss: 15761.040140\n",
      "Epoch 46, loss: 15760.670070\n",
      "Epoch 47, loss: 15760.823913\n",
      "Epoch 48, loss: 15760.603310\n",
      "Epoch 49, loss: 15760.766114\n",
      "Epoch 50, loss: 15760.651661\n",
      "Epoch 51, loss: 15760.799840\n",
      "Epoch 52, loss: 15761.034165\n",
      "Epoch 53, loss: 15760.720528\n",
      "Epoch 54, loss: 15760.744403\n",
      "Epoch 55, loss: 15760.761439\n",
      "Epoch 56, loss: 15760.668928\n",
      "Epoch 57, loss: 15760.488008\n",
      "Epoch 58, loss: 15760.886194\n",
      "Epoch 59, loss: 15760.722910\n",
      "Epoch 60, loss: 15760.745816\n",
      "Epoch 61, loss: 15760.622607\n",
      "Epoch 62, loss: 15760.694854\n",
      "Epoch 63, loss: 15760.692594\n",
      "Epoch 64, loss: 15760.867009\n",
      "Epoch 65, loss: 15760.581419\n",
      "Epoch 66, loss: 15760.670210\n",
      "Epoch 67, loss: 15760.772088\n",
      "Epoch 68, loss: 15760.901897\n",
      "Epoch 69, loss: 15760.434130\n",
      "Epoch 70, loss: 15760.655309\n",
      "Epoch 71, loss: 15760.584297\n",
      "Epoch 72, loss: 15760.392808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, loss: 15760.721794\n",
      "Epoch 74, loss: 15760.430948\n",
      "Epoch 75, loss: 15761.005070\n",
      "Epoch 76, loss: 15760.716103\n",
      "Epoch 77, loss: 15760.382490\n",
      "Epoch 78, loss: 15760.826295\n",
      "Epoch 79, loss: 15760.364428\n",
      "Epoch 80, loss: 15760.480046\n",
      "Epoch 81, loss: 15760.490389\n",
      "Epoch 82, loss: 15760.416858\n",
      "Epoch 83, loss: 15760.445607\n",
      "Epoch 84, loss: 15760.676749\n",
      "Epoch 85, loss: 15760.311533\n",
      "Epoch 86, loss: 15760.143434\n",
      "Epoch 87, loss: 15760.667555\n",
      "Epoch 88, loss: 15760.650848\n",
      "Epoch 89, loss: 15760.208885\n",
      "Epoch 90, loss: 15760.361161\n",
      "Epoch 91, loss: 15760.660448\n",
      "Epoch 92, loss: 15760.656667\n",
      "Epoch 93, loss: 15760.427102\n",
      "Epoch 94, loss: 15760.524994\n",
      "Epoch 95, loss: 15760.300144\n",
      "Epoch 96, loss: 15760.809594\n",
      "Epoch 97, loss: 15760.873090\n",
      "Epoch 98, loss: 15760.443365\n",
      "Epoch 99, loss: 15760.216747\n",
      "Epoch 100, loss: 15760.326359\n",
      "Epoch 101, loss: 15760.642794\n",
      "Epoch 102, loss: 15760.192521\n",
      "Epoch 103, loss: 15759.931638\n",
      "Epoch 104, loss: 15759.989957\n",
      "Epoch 105, loss: 15759.885318\n",
      "Epoch 106, loss: 15760.286060\n",
      "Epoch 107, loss: 15760.461013\n",
      "Epoch 108, loss: 15760.421803\n",
      "Epoch 109, loss: 15760.378854\n",
      "Epoch 110, loss: 15760.497127\n",
      "Epoch 111, loss: 15760.573222\n",
      "Epoch 112, loss: 15760.210017\n",
      "Epoch 113, loss: 15760.315521\n",
      "Epoch 114, loss: 15760.090919\n",
      "Epoch 115, loss: 15760.146598\n",
      "Epoch 116, loss: 15760.069313\n",
      "Epoch 117, loss: 15759.854868\n",
      "Epoch 118, loss: 15760.271339\n",
      "Epoch 119, loss: 15760.038023\n",
      "Epoch 120, loss: 15760.061384\n",
      "Epoch 121, loss: 15760.202130\n",
      "Epoch 122, loss: 15760.056431\n",
      "Epoch 123, loss: 15760.192216\n",
      "Epoch 124, loss: 15760.026202\n",
      "Epoch 125, loss: 15760.068758\n",
      "Epoch 126, loss: 15760.111074\n",
      "Epoch 127, loss: 15760.110066\n",
      "Epoch 128, loss: 15760.038294\n",
      "Epoch 129, loss: 15759.864563\n",
      "Epoch 130, loss: 15760.026411\n",
      "Epoch 131, loss: 15760.296654\n",
      "Epoch 132, loss: 15760.047581\n",
      "Epoch 133, loss: 15760.308770\n",
      "Epoch 134, loss: 15760.035314\n",
      "Epoch 135, loss: 15759.840730\n",
      "Epoch 136, loss: 15759.943374\n",
      "Epoch 137, loss: 15759.936129\n",
      "Epoch 138, loss: 15759.656934\n",
      "Epoch 139, loss: 15759.948563\n",
      "Epoch 140, loss: 15760.003355\n",
      "Epoch 141, loss: 15760.157131\n",
      "Epoch 142, loss: 15760.117866\n",
      "Epoch 143, loss: 15759.952278\n",
      "Epoch 144, loss: 15760.036767\n",
      "Epoch 145, loss: 15760.117320\n",
      "Epoch 146, loss: 15759.531322\n",
      "Epoch 147, loss: 15759.836552\n",
      "Epoch 148, loss: 15760.065452\n",
      "Epoch 149, loss: 15759.820393\n",
      "Epoch 150, loss: 15759.935699\n",
      "Epoch 151, loss: 15759.800965\n",
      "Epoch 152, loss: 15759.823449\n",
      "Epoch 153, loss: 15759.782322\n",
      "Epoch 154, loss: 15759.696932\n",
      "Epoch 155, loss: 15759.881046\n",
      "Epoch 156, loss: 15759.944818\n",
      "Epoch 157, loss: 15759.876613\n",
      "Epoch 158, loss: 15759.725681\n",
      "Epoch 159, loss: 15759.663562\n",
      "Epoch 160, loss: 15759.516750\n",
      "Epoch 161, loss: 15759.742844\n",
      "Epoch 162, loss: 15760.155731\n",
      "Epoch 163, loss: 15759.776063\n",
      "Epoch 164, loss: 15759.732978\n",
      "Epoch 165, loss: 15759.716992\n",
      "Epoch 166, loss: 15759.722851\n",
      "Epoch 167, loss: 15759.998270\n",
      "Epoch 168, loss: 15759.827957\n",
      "Epoch 169, loss: 15759.661025\n",
      "Epoch 170, loss: 15759.617283\n",
      "Epoch 171, loss: 15759.554649\n",
      "Epoch 172, loss: 15759.678817\n",
      "Epoch 173, loss: 15759.515106\n",
      "Epoch 174, loss: 15759.353102\n",
      "Epoch 175, loss: 15759.579897\n",
      "Epoch 176, loss: 15759.911111\n",
      "Epoch 177, loss: 15759.815481\n",
      "Epoch 178, loss: 15759.685123\n",
      "Epoch 179, loss: 15759.519641\n",
      "Epoch 180, loss: 15759.400528\n",
      "Epoch 181, loss: 15759.303976\n",
      "Epoch 182, loss: 15759.497856\n",
      "Epoch 183, loss: 15759.404948\n",
      "Epoch 184, loss: 15760.046450\n",
      "Epoch 185, loss: 15759.524963\n",
      "Epoch 186, loss: 15759.674839\n",
      "Epoch 187, loss: 15760.262391\n",
      "Epoch 188, loss: 15759.744911\n",
      "Epoch 189, loss: 15759.390641\n",
      "Epoch 190, loss: 15759.550753\n",
      "Epoch 191, loss: 15759.146428\n",
      "Epoch 192, loss: 15759.285785\n",
      "Epoch 193, loss: 15759.263879\n",
      "Epoch 194, loss: 15759.586888\n",
      "Epoch 195, loss: 15759.420526\n",
      "Epoch 196, loss: 15759.422122\n",
      "Epoch 197, loss: 15759.148518\n",
      "Epoch 198, loss: 15759.456579\n",
      "Epoch 199, loss: 15759.290582\n",
      "best validation accuracy achieved: 0.240000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "best = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        best.append([classifier, accuracy])\n",
    "        \n",
    "best.sort(key=lambda x: x[1], reverse=True)\n",
    "best_classifier = best[0][0]\n",
    "best_val_accuracy = best[0][1]\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<linear_classifer.LinearSoftmaxClassifier at 0x20c8f676940>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.189000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
